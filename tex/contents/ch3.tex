\chapter{Related Work}\label{ch:ch3}
Given the understanding of both EMP and Transformers presented in section \ref{ch:ch2}, we'll \rtodo[,inline]{Try not to use contractions. Use we will instead of we'll} now give an overview of the existing relevant research from which we will build upon. This will include a variety of different EMP models, as well as applications of the Transformer to MIR related problems. \rtodo[,inline]{Put the data section in a different place}. 

\section{Existing Expressive Musical Performance Generation Models}
EMP generation models fit into one of two categories, rule-based and data-based. Rule-based systems are built using a set of hardcoded rules which are derived using pre-existing musical knowledge and empirical studies involving human cognition. Data-driven models rely on probabilistic and machine learning methods to take an existing dataset of both scores and performances and use the performance data as a guide to learn the mapping between score features and performance features. 

\subsection{Rule Based}
The KTH system \cite{friberg2006overview} sits at the center of rule-based EMP models. Development of the KTH started in the 1980s and continued well into the 21st century. The initial idea behind the KTH system was to define a set of rules relating to the structure of a musical composition and how they affect a resulting performance, specifically with singing synthesis. The first set of rules was developed related for use in singing synthesis, and these rules were then later adapted to general musical performance. 

Since then there have been two general methods in the continued development of the KTH rule system. The first is that of \emph{analysis-by-synthesis}, which involved using the rules to synthesize musical performances that were presented to human listeners (both professional and non-professional), gathering listening feedback, and then using this feedback to modify the rules where needed. The second was an \emph{analysis-by-measurement} method. This method uses direct computation to analyze the result of a computational generated performance with an existing real performance \footnote{This falls more in line with the data-driven approaches. However, data-driven models use the performance data to directly build the model, whereas the use of real performance data in the KTH system is for evaluation purposes only. Any further updates to the model still rely on a hardcoded set of rules}. Example rules from the KTH system are found in figure \ref{fig:kth-rules} \rtodo{May need more exploration in caption}. 

\begin{figure}
    \centering
    \missingfigure{show some of the rules from the KTH paper in either a table or a figure}
    \caption{The left column shows the name of the rule, and the right column provides a language description of that rule. These are the rules that we might expect a data-based system to learn.}
    \label{fig:kth-rules}
\end{figure}

To our knowledge, the KTH rule-based system is the first sophisticated computational model for generating expressive performance, and its methods form the basis for much of the research that has been conducted since then. The explicitly defined rules in the KTH system can be thought of as the rules we might expect a data-based model to learn. \citet{widmer2002machine} shows that data-driven methods do in fact learn some of the same rules as the KTH system, but also can learn rules that are the opposite of KTH rules. As has already been discussed, the difficult nature of model evaluation may describe this phenomenon, as there is no telling which rule is more "correct" than another. Nevertheless, the KTH rule system has been an important milestone in the development of EMP models in general. 

\subsection{Data Based}\label{sec:data-based}
State of the art EMP generation models rely on existing data of actual human performance to learn the mapping between score and performance. The state of the art models are generally based either on sequential probabilistic or non-linear neural network methods\cite{cancino2018computational}, although there has been previous work with linear and non-sequential modeling. A complete overview of all relevant EMP generation models is presented in \cite{cancino2018computational} and we will not iterate them here. Instead we will describe a few models and frameworks which are relevant to our work

\subsubsection{Basis Function Models}
The first of these is a complete computational and mathematical framework for exploring EMP, and is known as the Basis Model (BM) framework\cite{eduardo2018computational}. The BM framework for EMP describes the full end-to-end process involved both the generation and analysis of musical performance, starting with a set of Basis Function Models which are used to provide score features. The BM framework also defines \emph{expressive parameters}, which are analogous to our definition of performance features as outlined in \ref{sec:performance}. Given score features which are defined by a set of basis functions as well a set of expressive parameters used to numerically define a performance, the BM framework then defines a model which can map between the score features and expressive parameters. \rtodo[,inline]{This idea needs more cohesion with the rest of the thesis. Try to provide our own mathematical definition of EMP (similarly to the way we did with neural machine translation). We could actually use the BM framework as this definition, although it may be more mathematical than we need}. 

\citet{eduardo2018computational} outlines the full mathematical definition of the BM framework, as well as the evolution of the framework and its application with specific feature and model definitions. BM models first started as simple linear non-sequential models which learned the linear relationship between a set of defined basis functions (or score feature) \rtodo{add more information about score feature} and a single expressive parameter, such as MIDI velocity. This version of the BM models each expressive parameter independently from all others, and implies that the interpretation one expressive parameter will not have an effect on the other \rtodo{Verify that footnote is correct}. \footnote{Although this is not necessarily the case in actual performance, it is a simplifying mathematical trait that makes the development and interpretation of the models simpler. All of the BM models operate under this same assumption}. Both standard least squares regression and a probabilistic Bayesian approach are used to model the linear relationship. 

As the BM framework progressed, both non-linear and sequential models were introduced in the form of deep neural networks. The non-linear model was implemented first in the form of Feed-Forward Neural Network (FFNN) was implemented first and showed an increase in goodness-of-fit as well as predictive accuracy over the standard linear models. After the FFNN came a standard RNN and was used in conjunction with the FFNN with features where time-dependent and the sequential nature of music was relevant. The recurrent non-linear model performed the best relative to all other models. 

\subsubsection{virtuosoNet}
\rtodo[,inline]{Change virtuosoNet heading to look better. Also look into creating a macro for virtuosoNet to create a typeset so that the name stands out}. 
Similarly to the BM framework, the development of virtuosoNet is gradual. The first version of the model presented in \cite{jeong2018virtuosonet} uses a recurrent hierarchical attention network (HAN) along with a novel encoder-decoder architecture specific to the EMP domain. No quantitative or qualitative evaluation results are presented at this point. The next iteration of virtuosoNet uses a similar encoder-decoder architecture but introduces an iterative sequential graph-based neural network (ISGN) that relies on the score representation as a graph data structure \cite{jeong2019graph}. The latest version presented in \cite{jeong2019virtuosonet} returns to the HAN architecture, but does so with a larger dataset as well as additional more abstract hierarchical models that are hypothesized to create better structure at the metrical level and preserve patterns across mid-level structures of the composition, in addition to learning them at the low-level.  

Both the ISGN\cite{jeong2019graph} and HAN\cite{jeong2019virtuosonet} version of virtuosoNet are trained on the same dataset (which we will describe in section \rtodo{add section}) and evaluated quantitatively using MSE and qualitatively using listening tests. In terms of quantitative evaluation, both the ISGN and HAN perform better than baseline models which remove some of the architecture complexity related to hierarchical layers. The final version of HAN reports better MSE metrics than ISGN. The qualitative evaluation with listening tests shows that both ISGN and HAN perform better than baseline models as well as better than the "deadpan" performance, which is a performance model that is statically computed using a simple set of rules and gives a somewhat robotic-sounding performance \rtodo[,inline]{Provide more explanation for the deadpan recording. May be worth it to mention in the qualitative evaluation section}. The final HAN version's qualitative evaluation includes a comparison between the HAN and the publicly available version of the BM framework model \footnote{The website for the BM model can be found \href{here}{https://basismixer.cp.jku.at/static/app.html}. At the time of this writing, the website is currently unavailable}. 

The results in \cite{jeong2019virtuosonet} show that the HAN performs better than the BM model. There are many plausible reasons that may explain the difference in results other than the HAN being a superior model to the BM, including differences in the training data for both models, bias of the qualitative method towards the HAN, and the fact that the opinion of the members of the listening test doesn't necessarily imply one model being "superior" to another. However, given the results presented by \citet{jeong2019virtuosonet}, we will assume that this version of the HAN represents the current "state of the art" in the field, if such a thing even exists. 

\rtodo[,inline]{Add section that talks about the features used for virtuosoNet}

\section{Datasets}
One of the problems facing EMP and MIR in general is the lack of high quality and high scale datasets\cite{cancino2018computational}. This is in large part due to the fact that scope of possible data to collect related to music data is large, compared to other domains. As has already been discussed, there are different stages in the musical process, and each of them contain different possibilities for the representation of music. For example, composition can contain largely the same amount of information in at least three forms. The first and most common is the symbollic representation in the form of a data format like MusicXML. A musical performance also contains within it information about the composition itself, and performances can represented in an intermediate format such as MIDI, or in the form of raw audio. The same can be said for other fields such as NLP, which deals mostly with textual data, and Speech Processing, which deals mostly with language in the form of spoken word. However, the two fields are seen as distinct from each other and each come with more standardization in both research methods and data formats. Musical data and information has not seen the same rigour in the literature. 

Another inherent problem with getting high-quality musical datasets is that most of the readily available musical data comes in the form of audio, which is much more difficult to process than symbolic (MusicXML) or intermediate (MIDI) forms given that it contains large amounts of noise and does not necessarily compress musical information. In contrast, NLP and Computer Vision directly deal with text and image data respectively, which are both readily available at a large scale due to the internet. 

There are normally 3 required components for a EMP dataset. 
\begin{enumerate}
    \item Scores (usually in the form of MusicXML)
    \item Performances (usually in the form of MIDI)
    \item Metadata about the matching alignment between the score and performance. 
\end{enumerate}
Score data is usually gathered by finding readily available MusicXML files from open source software projects which contain music that is in the public domain (which all western classical music is) \footnote{\href{https://musescore.com}{MuseScore} is the most common. Also see the \href{https://imslp.org/wiki/Main_Page}{International Music Score Library Project}}, or by using Optical Music Recognition (OMR) to auotomatically scan paper sheet music into a digital form followed by manual corrections where needed. Because the relevant performance features are difficult to extract from raw audio, performance data usually comes in the form of MIDI. To gather MIDI data of professional performance, it is necessary for the performers to play on a computer-controlled piano which can record performances in MIDI form, as well as automatically play back recorded performances which allow the complete reproducibility of any existing performance. Both the Yamaha Disklavier \footnote{\url{https://usa.yamaha.com/products/musical_instruments/pianos/disklavier/index.html}} and the older Bosendorfer CEUS system have this capability. 

There is no standardized method for score-to-performance alignment methods and data representations. Each dataset presents in own alignment method as well as the metadata that represents the alignment. 

To provide context for the progression of data used in EMP generation, we'll start by touching an older dataset used in older EMP research, the Magaloff Corpus, and then describing a much larger scale dataset, the Piano-e-competition, which has recently been adapted for use in EMP generation. A full overview of datasets for EMP generation can be found in \cite{cancino2018computational}

\subsubsection{Magaloff Corpus}
Nikita Magaloff was a Russian pianist known for his performance cycles of Chopin's entire works for the solo piano. In one of his final cycles of performances recorder in 1989, he played on a Bosendorfer SE computer-controlled piano. The Magaloff Corpus \cite{flossmann2010magaloff} presented the recorded performances were converted to the standard MIDI format\cite{eduardo2018computational}, thus making available full performance data of all of Chopins compositions for solo piano. Score data was obtained using OMR with manual corrections where needed.  The alignment method presented in \cite{grachten2006expressivity} was used to produce the note-matching annotations, along with manual correction. The dataset contains over 10 hours of playing, 150 compositions, and over 320,000 performed notes. The corpus however, is not publicly available, and has only been used in research by \citet{flossmann2010magaloff} and colleagues \cite{eduardo2018computational}. 

\subsubsection{Piano-e-competition}
As has been discussed, there is a large push in modern MIR to produce high-quality large datasets. At the heart of this research in MIR is the Piano-e-competition. Started in 2002, it is an international piano competition which attracts some of the promising up and coming musicians at both the senior and junior level \cite{the-disklavier-education-network}. Every performance from the competition is played on a Yamaha Disklavier. Every performance from the competition dating back to 2002 is recorded in both MIDI and audio. \citet{hawthorne2018enabling} introduce the MAESTRO dataset, which presents both MIDI and audio data from the Piano-e-competition in a canonical and easily accessible form. The dataset was first used to build a full musical analysis and generation process framework named wav2midi2wave, which includes a musical transcription process \cite{hawthorne2017onsets} from raw audio to midi (wav2midi), a direct musical composition and performance generation model \cite{huang2018music} \footnote{This model directly generates MIDI files without using scores. It simultaneously generates a composition and performance. This task can been seen as a merging of the two separate tasks, composition and performance, as show in figure \ref{fig:generation_process}} (can be seen as the midi or midi2midi part of the wav2midi2wav framework), and a synthesis model that takes MIDI and generates raw audio\cite{oord2016wavenet} (midi2wav). 

The Piano-e-competition also forms the basis for the data used to train virtuosoNet. The Piano-e-competition dataset itself does not provide any data about the scores of the music being performed; this data was collected by \citet{jeong2019virtuosonet} mostly from MuseScore. On top of gathering the score data for all performances in the Piano-e-competition, \citet{jeong2019virtuosonet} also run the automatic score-to-performance alignment algorithm presented in \cite{nakamura2017performance} to provide metadata about the alignment between each score and performance. Score-to-performance alignment is error-prone (especially in the case of performance mistakes) and as result, there are some performance notes which are not aligned to those in a score. \citet{jeong2019virtuosonet} also perform additional manual and heuristic corrections to the alignment where needed. The major difference between the Piano-e-competition and the Magaloff corpus is that it contains multiple performances for the same score, whereas the Magaloff Corpus has a 1-1 mapping between a score and performance. The dataset presented in \cite{jeong2019virtuosonet} has 226 scores across 16 different composers, roughly 660,000 score notes, and around 3,500,000 performance notes. The number of matched performance notes is ten times larger than the Magaloff Corpus, and all data is made publicly available \footnote{The dataset can be found at \url{https://github.com/mac-marg-pianist/chopin_cleaned}}. 

The Aligned Scores and Performances (ASAP) dataset \cite{foscarin2020asap} is a recent adaptation of both the dataset presented by \citet{jeong2019virtuosonet} and the MAESTRO dataset. It uses the MusicXML files from \citet{jeong2019virtuosonet}, audio from the MAESTRO dataset, and MIDI files from both sources which are extracted from the common source of the Piano-e-competition. It provides additional alignment metadata for both MIDI and audio fields, as well as more manual correction in the MusicXML score files. Although the purpose of the ASAP dataset is for Automatic Music Transcription (AMT), which is the task of transcribing a score from a performance (either in audio or MIDI form) \footnote{AMT can be seen as the "opposite" of EMP, given that transcription does the reverse process of performance by mapping a performance to a score}, it is just as equally useful for EMP generation. To our knowledge it has not been used in any EMP generation task. Although it is largely similar to the dataset used to train virtuosoNet, the implications of it's extensions have yet to be determined in EMP. 
