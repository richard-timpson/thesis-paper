\chapter{Background: Sequential Modeling}\label{ch:ch3}

This project's motivation comes from the context set by the current state of the art of EMP generation and ML in general. In the following chapter, we provide some of this context and its relevance to our problem.  Current state of the art models in performance generation use Recurrent Neural Networks as their foundation. Recent developments in neural sequence modeling move entirely away from RNNs and toward a new family of ANN architectures, the Transformer. Because Transformers have not seen application in EMP generation models, they are the focus of our work. 

\section{Sequential Data}\label{sec:sequential-data}
One fundamental aspect of modern machine learning is modeling sequential data. Sequential data consists of individual data points with a relationship to each other according to some specific order and position in time. A simple example of sequential data is the weather, which follows predictable patterns according to the time of year. Musical data is also fundamentally sequential\cite{widmer2016getting} given that we experience music as events that happen in time, and the relationship of such events according to their position in time is paramount to the phenomena of musical experience. Language and speech exhibit this same property, and the research in natural language processing drives much of the research advance in neural sequential data modeling. 

Sequential data modeling is typically categorized into different tasks. Perhaps the most common task is sequence classification, which seeks to assign some sequence of data points to a particular class of data. In this case, input data is defined as $X = \{x_1, x_2, x_3, ..., x_n\}$ where $x_i \in \mathbb{R}^m$. Output data is a single value $y \in L$ where $L$ is a set of class labels. A sequence classification model $C: \mathbb{R}^{n \times m} \rightarrow L$ will then map from an input sequence $X$ to a class label $y$. Email spam detection and genre classification are common use cases of sequence classification models in NLP and MIR, respectively. 

% seq for seq2seq
\newcommand{\seq}{\emph{seq2seq}}

EMP generation is a more complicated process. It involves mapping an input sequence (score) to another output sequence (performance). We call such a task a sequence-to-sequence (\seq{}) model. In this case our input data $X$ is the same, but our output data is also defined as a sequence of vectors $Y = \{y_1, y_2, y_3, ..., y_{\hat{n}}\}$, where $y_i \in \mathbb{R}^{\hat{m}}$. We can then define a \seq{} model as $S: \mathbb{R}^{n \times m} \rightarrow \mathbb{R}^{\hat{n} \times \hat{m}}$ which will produce an output sequence $Y$ given the input sequence $X$. In EMP generation, $m$ is the number of score features, $n$ is the number of input score notes, $\hat{m}$ is the number of performance features, and $\hat{n}$ the number of output performance notes. 

\section{Case Study: Neural Machine Translation}
RNNs and their common adaptations as an LSTM have historically been the default modeling choices for sequential data Deep Learning footenote{For brevities sake, we do not provide the detailed mathematical definition for RNNs here and refer to the reader to \cite{goodfellow2016deep}}. However, in recent years the Transformer\cite{vaswani2017attention} architecture model has outperformed RNNs in many tasks and is becoming the de-facto standard for sequential data modeling in modern Machine Learning. To provide context for the origin of the Transformer, we will outline the historical progress of an NLP task known as neural machine translation (NMT). 

Machine translation (MT) is the task of computationally translating one natural language to another\footnote{\href{https://translate.google.com/}{Google Translate} is one successful commercial application.}. Traditional MT systems relied on complicated rule sets and decoding algorithms stitched together to create a statistical model known as a statistical machine translation (SMT) system. Rather than an amalgamation of several different systems developed independently, NMT systems are trained end-to-end inside a single ANN architecture and significantly reduce building MT models' complexity. NMTs are \seq{} models and typically operate by translating a single sentence at a time. Until the advent of the Transformer, NMT models primarily used RNNs. 

% ed for encoder decoder
\newcommand{\ed}{\emph{encoder-decoder}}
One of the complexities in building an NMT model is that the source and target sentences are often not the same lengths - $n \neq l$ in our definition of \seq{} models given in section \ref{sec:sequential-data}. NMT translation systems use what is known as an \ed{} architecture, which account for the variable-length input and output sequences. From a probabilistic perspective, the job of the \ed{} architecture is to model the conditional probability distribution of a variable-length output sentence $Y$ given a variable-length input sequence $X$. $$ P(y_1, y_2, ... y_n | x_1, x_2, ..., x_l)$$ In the \ed{} architecture, calculation of the distribution is decomposed into two separate models. The encoder's job is to read in the source sequence and find a good representation, or encoding, of that sequence. In the original formulation of the \ed{} NMT architecture, \citet{cho2014learning} present this encoding in the form of a fixed size vector $c$. The decoder is an autoregressive language model\footnote{Autoregressive models are sequence-based models that take as input the output of the model at previous time steps. Language models are instances of autoregressive models that are capable of generating novel texts of varying lengths. Language models can generate novel text from scratch, although they are often prompted with existing text to write in a particular style or on a certain subject. See \href{https://transformer.huggingface.co/doc/distil-gpt2}{this online playground for an example}}, and uses $c$ as a condition to \emph{generate} the new sentence in the target language. Using this decomposition, we can view the decoder as calculating the probability of the next word in the sentence given all of the previous words, and the hidden encoding vector \cite{bahdanau2014neural}. $$P(Y) = \prod_{\hat{n}=1}^{n}P(y_i \vert y_1, y_2, ..., y_{i-1}, c)$$

\newcommand{\at}[1]{\emph{#1}}

This RNN based \ed{} model improved upon the state of the art results for existing SMT based translation systems. However, there is an inherent limit imposed on the system for long sentences. \citet{cho2014learning} shows that the performance of a basic \ed{} model deteriorates rapidly as the length of an input sentence increases. To account for this 
\citet{bahdanau2014neural} present what is known as the \at{attention} mechanism. Instead of using a fixed-sized vector encoding, \at{attention} allows the decoder model to search for a set of positions in the source sentence where the most relevant information is concentrated and uses this information as it generates text in the target language. In simpler terms, the decoder "pays attention" to the source sentence's most relevant words to find the right translation of every word at each time step. 

Rather than use a fixed-length vector at every time step, the decoder is modeled as follows $$p(y_t \vert y_1, y_2, ..., y_{t-1}, X) = g(y_{i-1}, s_i, c_i)$$ where $g$ is some non-linear potentially multi-layered function that outputs the probability of $y_{t}$ and $s_{\hat{t}}$ is the hidden state of the RRN at time step $t$. $c_i$ is a context vector that is generated using information about the relationship between certain words in the source sentence and the next word $y_t$ to be generated (see \citet{bahdanau2014neural} for the full description behind this context vector). The concept of attention is built into a set of context vectors for each time step in the generation process. By getting rid of the restriction of using a single fixed-size vector, this attention-based model achieved state of the art results in NMT. 

Since the introduction of the attention mechanism, it has been used in tandem with RNNs and other DL models to push state of the art in various sequence-based tasks, such as Question Answering, Sentiment Analysis, and Part-of-Speech tagging \cite{chaudhari2019attentive}. There a several reasons for the increase in performance; one reason being that attention allows the models to better the dependencies of the sequence without respect to their distance from each other (this is a known limitation of the RNNs) \cite{vaswani2017attention}, another being that attention helps with parallelization in training because of the lack of constraint on the dependency of one time step to another as is the case with RNNs. In almost all cases, the attention mechanism was used in conjunction with other modeling architectures. It wasn't until the introduction of a completely new family of NN architectures, Transformers, that the attention mechanism was applied outside of any existing network architecture.


\section{Transformers}
\rtodo[,inline]{transformer section needs a lot of work given the change of the general document outline}
To properly understand the significance of Transformers and their involvement in our work, it is necessary to provide context about the domain in which the Transformer was first introduced and give an overview of the existing work in that domain that the Transformer built on. We'll then provide some detail about the Transformer itself as well as adaptations of the original architectures and their results. 

% \subsection{Natural Language Processing and Machine Translation}
% One of the most commonly studied fields in Machine Learning and Artificial Intelligence is Natural Language Process (NLP), which (similarly to MIR) uses computation to ascertain a better understanding of human language as well as build technological tools that are useful in performing common language tasks. One such task is that of machine translation, which involves using computation alone to translate text from one language to another. NLP research usually invovles building sequence-based models (which explore the individual elements of an ordered set of items) due to the inherently sequential nature of language, as opposed to a non-sequential model which doesn't account for sequential data, such as a single image. Machine translation falls under the category of sequence-to-sequence (seq-2-seq) modeling problems, which involve the mapping and relationship of one sequence to another. This is typically in the form of translating a single sentence from one language (English) to another (French).

\newcommand{\mb}[1]{\mathbf{#1}}

% More specifically, machine translation (and other seq-2-seq tasks) can be defined as taking an input sequence $\mb{x} = \{x_1, x_2, x_3,...x_m\}$ of size m and producing an output sequence $\mb{y} = \{y_1, y_2, y_3, ... y_n\}$ of size n such that $M(\mb{x}) = \mb{y}$, where $M$ can be any machine translation model. In some seq-2-seq tasks, $m = n$ are the same, implying that the input and output sequence are the same length. As is often the case in language translation, the input sentence and output sentence are of varying lengths, so we can assume that $m \neq n$. 

% It is common to use an encoder-decoder architecture for $M$, where there is an encoder $E$ which takes in the input data and outputs and finds some hidden representation $E(\mb{x} = \mb{z})$. This hidden representation is given as input to the decoder, and the decoder uses it to produce the final output, $D(\mb{z}) = \mb{y}$. We can then define an encoder-decoder seq-2-seq model as $M(x) = D(E(\mb{x})) = \mb{y}$. Historically, a Long-Short-Term-Memory neural network (LSTM)\footnote{An LSTM is a common variant of a Recurrent Neural Network (RNN) which is the most standard deep learning model used for sequence modeling. See \url{https://en.wikipedia.org/wiki/Long_short-term_memory}} has been used for both $E$ and $D$, where the hidden representation $\mb{z}$ has been a fixed length vector \rtodo{Add reference}. 

% One of the limitations of such a model is that it has to compress all of the information of the input data into the fixed-length vector $\mb{z}$ which causes the network to potentially lose important information, particularly in the case where an input sentence is given to the network which is longer than any present in the training data. \citet{bahdanau2014neural} present the attention mechanism which, used in conjunction with an RNN based encoder, allows for the hidden representation to itself be a sequence $\mb{z} = \{z_1, z_2, z_3, ..., z_m\}$ of size $m$ (the same size as the input sequence).    Each $z_i$ element in the sequence contains information about the whole input sequence, with an emphasis on the elements closest to the $i$-th element. This allows the hidden representation to encode any relationship that one element in the sequence has with another. The decoder then uses this information to "pay attention" to words in the output sequence that have a relationship with words in the input sequence, given the context that is encoded in the hidden representation at a particular time step $i$. The attention mechanism and model that uses it achieved state of the art results in the machine translation task, due in part to the fact that hidden representation is not limited to a fixed-size vector. The original attention mechanism presented by \citet{bahdanau2014neural} and its adaptations have since been used in tandum with recurrent models to improve the state of the art in several sequence modeling tasks \rtodo{find reference}. One of the limitations with standard recurrent networks is their inability to retain information across long sequences - attention provides a way to create additional context and better memory across these longer sequences which has led to the increase of performance in attention-based models. \rtodo{find reference}. 

\subsection{Attention is All You Need}

In the seminal paper, \citet{vaswani2017attention} introduce the Transformer. The Transformer is an encoder-decoder seq-2-seq modeling neural network architecture that relies solely on the use of attention and cuts out any semblance of a recurrent architecture. The Transformer was the first architecture to make use of attention by itself, and by doing so pushed the state of the art in machine translation even further than it had been with attention-based recurrent models. 

The Transformer architecture consists of a stack of $N$ layers, all of which use a combination of a self-attention (attention that applies only within a single input sequence and not between an input and a output sequence) \rtodo[,inline]{Explore different ways to describe self-attention. May not even be necessary at all to mention} mechanism along with a standard pointwise fully connected feed-forward neural network (FFNN). Both the encoder and decoder comprise of these attention based stacked layers. For a full description of the architecture see \cite{vaswani2017attention}. 

\subsection{Transformer Adaptations: BERT and GPT}
Of particular interest in the new Transformer modeling domain is powerful adaptations of the original architecture which have been applied to many other NLP tasks besides machine translation. On such architecture, BERT (which stands for Bidirectional Encoder Representations from Transformers), uses what can be referred to as an "encoder only" Transformer model. 

The original Transformer was built with machine translation in mind, but there are several other NLP tasks that could possibly benefti from using an attention only architecture. Some of these tasks include standard text classification, textual entailment, sentiment analysis, question answering, and many more \rtodo{Find reference}. BERT was introduced as an encoder only transformer model that could generalize to all of these tasks. The method which it made use of was pre-training the model on a massive data set, with the intuition that by feeding the model so much data that it would learn a general representation of language that could then be applied to several different tasks. BERT is effectively a massive encoder for language in general, and can be used in conjuction with other models as simple decoders to perform these tasks. See the original paper\cite{devlin2018bert} for the full architecture and details. 

Similarly to BERT, the Generative Pre-trained Transformer (GPT) architecture\cite{radford2019language} is an adaptation of the original Transformer. The GPT architecture can be seen as a "decoder only" transformer, and is used as a general Language Model (LM). The task of a LM is simple; to predict the next word in a sequence of given words. Given that GPT is a generative model, it employes the decoder side of the Transformer, which is responsible for actually generating the text as part of the machine translation taks. Similarly to BERT, GPT models are pre-trained on massive amounts of data to learn a general representation of language, and used in conjuction with other models to perform various tasks. 

Both BERT and GPT have significatly pushed the state of art in NLP and sequence modeling in general. Their success in the domain of language presents questions about their effectiveness in other related domains, such as music. 


 