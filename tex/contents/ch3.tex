\chapter{Background: Sequential Modeling}\label{ch:ch3}
To understand the motivation for this project, it is necessary to provide some background and context on the current state of the art in the field. We provide this context not only for musical models, but also for all sequential data models in general. Current state of the art models in performance generation use as their foundation a Recurrent Neural Network which is designed to handle sequential data. There have been new recent developments in neural sequence modeling which move completely away from RNNs and towards a new family of ANN architectures, the Transformer. To our knowledge, Transformer models have not been used in EMP generation, which is the driving purpose behind our work. 

\section{Sequential Data}\label{sec:sequential-data}
One fundamental aspect in modern machine learning is to learn how to work with data which is sequential; meaning, data whose individual data points have some relationship with each other according to some specific order, or position in time. A simple example of sequential data is observations of weather, which follow predictable patterns according to the time of year. Musical data is fundamentally sequential\cite{widmer2016getting} given that we experience music as individual events across time. Language and speech also exhibit this same property, and much of the work in neural sequential data modeling is based upon natural language processing. 

There are different types tasks which are relevant to sequential data. Perhaps the most common task is that of classification, which seeks to assign some sequence of data points to a particular class of data. In this case, the input data is defined as $X = \{x_1, x_2, x_3, ..., x_n\}$ where $x_i \in \mathbb{R}^m$ is some $m$ dimensional vector. The output data is a single value $y \in L$ where $L$ is some set of class labels. A sequence classification model $C: \mathbb{R}^{n \times m} \rightarrow L$ will then map from some input sequence $X$ to a class label $y$ \rtodo[,inline]{Possibly add reference here to more detailed explanation of these features}. Email spam detection and genre classification are common use cases of sequence classification models in NLP and MIR, respectively. 

% seq for seq2seq
\newcommand{\seq}{\emph{seq2seq}}

EMP generation is a more complicated process however, and involves the mapping of the input sequence (score) to another output sequence (performance). We call such a task a sequence-to-sequence (\seq{}) model. In this case our input data $X$ is the same, but our output data is also defined as a sequence of vectors $Y = \{y_1, y_2, y_3, ..., y_l\}$, where $y_i \in \mathbb{R}^k$ is a k-dimensional vector. A \seq{} model $S: \mathbb{R}^{n \times m} \rightarrow \mathbb{R}^{l \times k}$ will map from an input sequence $X$ to an output sequence $Y$. For EMP generation, $m$ is the number of score features, $n$ is number of input score notes, $k$ is the number of performance features, and $l$ the number of output performance notes. 

\section{Case Study: Neural Machine Translation}
The Recurrent Neural Network and its adaptations have historically been used as the go to model for sequential modeling with NNs. For brevities sake, we do not provide the detailed mathematical defintion for RNNs here and refer to the reader to \cite{goodfellow2016deep}. However, in recent years the Transformer\cite{vaswani2017attention} architecture model has surpassed RNNs in many tasks as is becoming the de-facto standard for sequential data modeling in modern Machine Learning. To provide context for the origin of Transformer we will outline the historical progress of an NLP task which was the first application of the Transformer. That is neural machine translation (NMT). 

Machine translation (MT) is the task of computationally translating one natural language to another\footnote{\href{https://translate.google.com/}{Google Translate} is one successful commercial application.}. Traditional MT systems relied rule sets and complicated decoding algorithms, and are known as statistical machine translation (SMT). NMT allows for the end-to-end training of a translation system inside of a single ANN and significantly reduces the complexity of building MT models \cite{chu2018survey}. NMTs are \seq{} models and typically operate by translating a single sentence at a time. Until the advent of the Transformer, they were based on RNN models. 

% ed for encoder decoder
\newcommand{\ed}{\emph{encoder-decoder}}

One of the complexities in building a NMT model is that the source and target sentences are often not the same length - $n \neq l$ in our definition of \seq{} models given in section \ref{sec:sequential-data}. To account for this, NMT translation systems use what is known as an \ed{} architecture. The job of the encoder is to learn some some representation, or encoding, of the source sentence. In the original formulation of the \ed{} NMT architecture, \citet{cho2014learning} present this encoding was in the form of a fixed size vector $c$. This vector is then given to the decoder which is a language model, which is a generative sequence model that is auto-regressive (uses the output at one time step as the input for the next). The decoder uses $c$ as a condition to \emph{generate} the new sentence in the target language. From a probablistic perspective, we can view the decoder as calculating the probability of the next word in the sentence given all of the previous words as well as the hidden encoding vector. $$P(y_t \vert y_{t-1}, y_{t_2}, ..., y_1, c)$$

\newcommand{\at}[1]{\emph{#1}}

This RNN based \ed{} model was able to improve upon the state of the art results for existing SMT based translation system \. However, there is an inherent limit imposed on the system for long sentences. \citet{cho2014learning} shows that the performance of a basic \ed{} model deteriorates rapidly as the length of an input sentence increases. To account for this 
\citet{bahdanau2014neural} present what is known as the \at{attention} mechanism. Instead of using a fixed sized vector encoding, \at{attention} allows the model to search for a set of positions in the source sentence where the most relevant information is concetrated. \rtodo[,inline]{Continue discussion about Attention}


% Given the understanding of both EMP and Transformers presented in section \ref{ch:ch2}, we'll \rtodo[,inline]{Try not to use contractions. Use we will instead of we'll} now give an overview of the existing relevant research from which we will build upon. This will include a variety of different EMP models, as well as applications of the Transformer to MIR related problems. \rtodo[,inline]{Put the data section in a different place}. 

\section{Transformers}
\rtodo[,inline]{transformer section needs a lot of work given the change of the general document outline}
To properly understand the significance of Transformers and their involvement in our work, it is necessary to provide context about the domain in which the Transformer was first introduced and give an overview of the existing work in that domain that the Transformer built on. We'll then provide some detail about the Transformer itself as well as adaptations of the original architectures and their results. 

% \subsection{Natural Language Processing and Machine Translation}
% One of the most commonly studied fields in Machine Learning and Artificial Intelligence is Natural Language Process (NLP), which (similarly to MIR) uses computation to ascertain a better understanding of human language as well as build technological tools that are useful in performing common language tasks. One such task is that of machine translation, which involves using computation alone to translate text from one language to another. NLP research usually invovles building sequence-based models (which explore the individual elements of an ordered set of items) due to the inherently sequential nature of language, as opposed to a non-sequential model which doesn't account for sequential data, such as a single image. Machine translation falls under the category of sequence-to-sequence (seq-2-seq) modeling problems, which involve the mapping and relationship of one sequence to another. This is typically in the form of translating a single sentence from one language (English) to another (French).

\newcommand{\mb}[1]{\mathbf{#1}}

% More specifically, machine translation (and other seq-2-seq tasks) can be defined as taking an input sequence $\mb{x} = \{x_1, x_2, x_3,...x_m\}$ of size m and producing an output sequence $\mb{y} = \{y_1, y_2, y_3, ... y_n\}$ of size n such that $M(\mb{x}) = \mb{y}$, where $M$ can be any machine translation model. In some seq-2-seq tasks, $m = n$ are the same, implying that the input and output sequence are the same length. As is often the case in language translation, the input sentence and output sentence are of varying lengths, so we can assume that $m \neq n$. 

% It is common to use an encoder-decoder architecture for $M$, where there is an encoder $E$ which takes in the input data and outputs and finds some hidden representation $E(\mb{x} = \mb{z})$. This hidden representation is given as input to the decoder, and the decoder uses it to produce the final output, $D(\mb{z}) = \mb{y}$. We can then define an encoder-decoder seq-2-seq model as $M(x) = D(E(\mb{x})) = \mb{y}$. Historically, a Long-Short-Term-Memory neural network (LSTM)\footnote{An LSTM is a common variant of a Recurrent Neural Network (RNN) which is the most standard deep learning model used for sequence modeling. See \url{https://en.wikipedia.org/wiki/Long_short-term_memory}} has been used for both $E$ and $D$, where the hidden representation $\mb{z}$ has been a fixed length vector \rtodo{Add reference}. 

% One of the limitations of such a model is that it has to compress all of the information of the input data into the fixed-length vector $\mb{z}$ which causes the network to potentially lose important information, particularly in the case where an input sentence is given to the network which is longer than any present in the training data. \citet{bahdanau2014neural} present the attention mechanism which, used in conjunction with an RNN based encoder, allows for the hidden representation to itself be a sequence $\mb{z} = \{z_1, z_2, z_3, ..., z_m\}$ of size $m$ (the same size as the input sequence).    Each $z_i$ element in the sequence contains information about the whole input sequence, with an emphasis on the elements closest to the $i$-th element. This allows the hidden representation to encode any relationship that one element in the sequence has with another. The decoder then uses this information to "pay attention" to words in the output sequence that have a relationship with words in the input sequence, given the context that is encoded in the hidden representation at a particular time step $i$. The attention mechanism and model that uses it achieved state of the art results in the machine translation task, due in part to the fact that hidden representation is not limited to a fixed-size vector. The original attention mechanism presented by \citet{bahdanau2014neural} and its adaptations have since been used in tandum with recurrent models to improve the state of the art in several sequence modeling tasks \rtodo{find reference}. One of the limitations with standard recurrent networks is their inability to retain information across long sequences - attention provides a way to create additional context and better memory across these longer sequences which has led to the increase of performance in attention-based models. \rtodo{find reference}. 

\subsection{Attention is All You Need}

In the seminal paper, \citet{vaswani2017attention} introduce the Transformer. The Transformer is an encoder-decoder seq-2-seq modeling neural network architecture that relies solely on the use of attention and cuts out any semblance of a recurrent architecture. The Transformer was the first architecture to make use of attention by itself, and by doing so pushed the state of the art in machine translation even further than it had been with attention-based recurrent models. 

The Transformer architecture consists of a stack of $N$ layers, all of which use a combination of a self-attention (attention that applies only within a single input sequence and not between an input and a output sequence) \rtodo[,inline]{Explore different ways to describe self-attention. May not even be necessary at all to mention} mechanism along with a standard pointwise fully connected feed-forward neural network (FFNN). Both the encoder and decoder comprise of these attention based stacked layers. For a full description of the architecture see \cite{vaswani2017attention}. 

\subsection{Transformer Adaptations: BERT and GPT}
Of particular interest in the new Transformer modeling domain is powerful adaptations of the original architecture which have been applied to many other NLP tasks besides machine translation. On such architecture, BERT (which stands for Bidirectional Encoder Representations from Transformers), uses what can be referred to as an "encoder only" Transformer model. 

The original Transformer was built with machine translation in mind, but there are several other NLP tasks that could possibly benefti from using an attention only architecture. Some of these tasks include standard text classification, textual entailment, sentiment analysis, question answering, and many more \rtodo{Find reference}. BERT was introduced as an encoder only transformer model that could generalize to all of these tasks. The method which it made use of was pre-training the model on a massive data set, with the intuition that by feeding the model so much data that it would learn a general representation of language that could then be applied to several different tasks. BERT is effectively a massive encoder for language in general, and can be used in conjuction with other models as simple decoders to perform these tasks. See the original paper\cite{devlin2018bert} for the full architecture and details. 

Similarly to BERT, the Generative Pre-trained Transformer (GPT) architecture\cite{radford2019language} is an adaptation of the original Transformer. The GPT architecture can be seen as a "decoder only" transformer, and is used as a general Language Model (LM). The task of a LM is simple; to predict the next word in a sequence of given words. Given that GPT is a generative model, it employes the decoder side of the Transformer, which is responsible for actually generating the text as part of the machine translation taks. Similarly to BERT, GPT models are pre-trained on massive amounts of data to learn a general representation of language, and used in conjuction with other models to perform various tasks. 

Both BERT and GPT have significatly pushed the state of art in NLP and sequence modeling in general. Their success in the domain of language presents questions about their effectiveness in other related domains, such as music. 


\section{Existing Expressive Musical Performance Generation Models}
\rtodo[,inline]{Make this section its own chapter with edits to make it fit in the general story}
EMP generation models fit into one of two categories, rule-based and data-based. Rule-based systems are built using a set of hardcoded rules which are derived using pre-existing musical knowledge and empirical studies involving human cognition. Data-driven models rely on probabilistic and machine learning methods to take an existing dataset of both scores and performances and use the performance data as a guide to learn the mapping between score features and performance features. 

\subsection{Rule Based}
The KTH system \cite{friberg2006overview} sits at the center of rule-based EMP models. Development of the KTH started in the 1980s and continued well into the 21st century. The initial idea behind the KTH system was to define a set of rules relating to the structure of a musical composition and how they affect a resulting performance, specifically with singing synthesis. The first set of rules was developed related for use in singing synthesis, and these rules were then later adapted to general musical performance. 

Since then there have been two general methods in the continued development of the KTH rule system. The first is that of \emph{analysis-by-synthesis}, which involved using the rules to synthesize musical performances that were presented to human listeners (both professional and non-professional), gathering listening feedback, and then using this feedback to modify the rules where needed. The second was an \emph{analysis-by-measurement} method. This method uses direct computation to analyze the result of a computational generated performance with an existing real performance \footnote{This falls more in line with the data-driven approaches. However, data-driven models use the performance data to directly build the model, whereas the use of real performance data in the KTH system is for evaluation purposes only. Any further updates to the model still rely on a hardcoded set of rules}. Example rules from the KTH system are found in figure \ref{fig:kth-rules} \rtodo{May need more exploration in caption}. 

\begin{figure}
    \centering
    \missingfigure{show some of the rules from the KTH paper in either a table or a figure}
    \caption{The left column shows the name of the rule, and the right column provides a language description of that rule. These are the rules that we might expect a data-based system to learn.}
    \label{fig:kth-rules}
\end{figure}

To our knowledge, the KTH rule-based system is the first sophisticated computational model for generating expressive performance, and its methods form the basis for much of the research that has been conducted since then. The explicitly defined rules in the KTH system can be thought of as the rules we might expect a data-based model to learn. \citet{widmer2002machine} shows that data-driven methods do in fact learn some of the same rules as the KTH system, but also can learn rules that are the opposite of KTH rules. As has already been discussed, the difficult nature of model evaluation may describe this phenomenon, as there is no telling which rule is more "correct" than another. Nevertheless, the KTH rule system has been an important milestone in the development of EMP models in general. 

\subsection{Data Based}\label{sec:data-based}
State of the art EMP generation models rely on existing data of actual human performance to learn the mapping between score and performance. The state of the art models are generally based either on sequential probabilistic or non-linear neural network methods\cite{cancino2018computational}, although there has been previous work with linear and non-sequential modeling. A complete overview of all relevant EMP generation models is presented in \cite{cancino2018computational} and we will not iterate them here. Instead we will describe a few models and frameworks which are relevant to our work

\subsubsection{Basis Function Models}
The first of these is a complete computational and mathematical framework for exploring EMP, and is known as the Basis Model (BM) framework\cite{eduardo2018computational}. The BM framework for EMP describes the full end-to-end process involved both the generation and analysis of musical performance, starting with a set of Basis Function Models which are used to provide score features. The BM framework also defines \emph{expressive parameters}, which are analogous to our definition of performance features as outlined in \ref{sec:performance}. Given score features which are defined by a set of basis functions as well a set of expressive parameters used to numerically define a performance, the BM framework then defines a model which can map between the score features and expressive parameters. \rtodo[,inline]{This idea needs more cohesion with the rest of the thesis. Try to provide our own mathematical definition of EMP (similarly to the way we did with neural machine translation). We could actually use the BM framework as this definition, although it may be more mathematical than we need}. 

\citet{eduardo2018computational} outlines the full mathematical definition of the BM framework, as well as the evolution of the framework and its application with specific feature and model definitions. BM models first started as simple linear non-sequential models which learned the linear relationship between a set of defined basis functions (or score feature) \rtodo{add more information about score feature} and a single expressive parameter, such as MIDI velocity. This version of the BM models each expressive parameter independently from all others, and implies that the interpretation one expressive parameter will not have an effect on the other \rtodo{Verify that footnote is correct}. \footnote{Although this is not necessarily the case in actual performance, it is a simplifying mathematical trait that makes the development and interpretation of the models simpler. All of the BM models operate under this same assumption}. Both standard least squares regression and a probabilistic Bayesian approach are used to model the linear relationship. 

As the BM framework progressed, both non-linear and sequential models were introduced in the form of deep neural networks. The non-linear model was implemented first in the form of Feed-Forward Neural Network (FFNN) was implemented first and showed an increase in goodness-of-fit as well as predictive accuracy over the standard linear models. After the FFNN came a standard RNN and was used in conjunction with the FFNN with features where time-dependent and the sequential nature of music was relevant. The recurrent non-linear model performed the best relative to all other models. 

\subsubsection{virtuosoNet}
\rtodo[,inline]{Change virtuosoNet heading to look better. Also look into creating a macro for virtuosoNet to create a typeset so that the name stands out}. 
Similarly to the BM framework, the development of virtuosoNet is gradual. The first version of the model presented in \cite{jeong2018virtuosonet} uses a recurrent hierarchical attention network (HAN) along with a novel encoder-decoder architecture specific to the EMP domain. No quantitative or qualitative evaluation results are presented at this point. The next iteration of virtuosoNet uses a similar encoder-decoder architecture but introduces an iterative sequential graph-based neural network (ISGN) that relies on the score representation as a graph data structure \cite{jeong2019graph}. The latest version presented in \cite{jeong2019virtuosonet} returns to the HAN architecture, but does so with a larger dataset as well as additional more abstract hierarchical models that are hypothesized to create better structure at the metrical level and preserve patterns across mid-level structures of the composition, in addition to learning them at the low-level.  

Both the ISGN\cite{jeong2019graph} and HAN\cite{jeong2019virtuosonet} version of virtuosoNet are trained on the same dataset (which we will describe in section \rtodo{add section}) and evaluated quantitatively using MSE and qualitatively using listening tests. In terms of quantitative evaluation, both the ISGN and HAN perform better than baseline models which remove some of the architecture complexity related to hierarchical layers. The final version of HAN reports better MSE metrics than ISGN. The qualitative evaluation with listening tests shows that both ISGN and HAN perform better than baseline models as well as better than the "deadpan" performance, which is a performance model that is statically computed using a simple set of rules and gives a somewhat robotic-sounding performance \rtodo[,inline]{Provide more explanation for the deadpan recording. May be worth it to mention in the qualitative evaluation section}. The final HAN version's qualitative evaluation includes a comparison between the HAN and the publicly available version of the BM framework model \footnote{The website for the BM model can be found \href{here}{https://basismixer.cp.jku.at/static/app.html}. At the time of this writing, the website is currently unavailable}. 

The results in \cite{jeong2019virtuosonet} show that the HAN performs better than the BM model. There are many plausible reasons that may explain the difference in results other than the HAN being a superior model to the BM, including differences in the training data for both models, bias of the qualitative method towards the HAN, and the fact that the opinion of the members of the listening test doesn't necessarily imply one model being "superior" to another. However, given the results presented by \citet{jeong2019virtuosonet}, we will assume that this version of the HAN represents the current "state of the art" in the field, if such a thing even exists. 

\rtodo[,inline]{Add section that talks about the features used for virtuosoNet}
