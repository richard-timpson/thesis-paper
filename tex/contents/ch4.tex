\chapter{Related Work: Music Generation Models}\label{ch:ch4}
As in other fields, state of the art models in music generation are mostly comprised of ANN models. Some music generation tasks have already seen a Transformer application, while others (such as expressive performance generation) have not. We will provide an overview of the existing state of the art in EMP generation and transformer-based music generation models. We also give some background into the data representation and feature engineering of each model. 

\section{Existing Expressive Musical Performance Generation Models}
EMP generation models fit into one of two categories, rule-based and data-based. Rule-based systems use hardcoded rules derived using pre-existing musical knowledge and empirical studies involving human cognition. Data-driven models rely on probabilistic and machine learning methods to take an existing dataset of both scores and performances and use the performance data as a guide to learn the mapping between score features and performance features. 

\subsection{Rule Based}
The KTH system \cite{friberg2006overview} sits at the center of rule-based EMP models and lays the foundation for all EMP generation models. Development of the KTH started in the 1980s and has continued well into the 21st century. The KTH system's initial methodology defined rules relating to musical composition structure and how it affects a resulting performance. The first set of rules applied specifically to singing synthesis were later adapted to general musical performance. 

Since then, there have been two general methods in the continued development of the KTH rule system. The first is that of \emph{analysis-by-synthesis}, which involved using the rules to synthesize musical performances presented to human listeners (both professional and non-professional), gathering listening feedback, and then using this feedback to modify the rules where needed. The second was an \emph{analysis-by-measurement} method. This method uses direct computation to evaluate a computational generated performance by comparing it with an existing real performance \footnote{This evaluation method is consistent with the data-driven approaches but generally applies to any generative model. Data-driven models use the performance data to directly build the model, whereas real performance data in the KTH system is for evaluation purposes only. Any further updates to the model still rely on a hardcoded set of rules}. Example rules from the KTH system are found in figure \ref{fig:kth-rules} \rtodo{May need more exploration in caption}. 

\begin{figure}
    \centering
    \missingfigure{show some of the rules from the KTH paper in either a table or a figure}
    \caption{The left column shows the name of the rule, and the right column provides a language description of that rule. These are the rules that we might expect a data-based system to learn.}
    \label{fig:kth-rules}
\end{figure}

To our knowledge, the KTH rule-based system is the first sophisticated computational model for generating expressive performance. The explicitly defined rules in the KTH system may be those we expect a data-based model to learn. \citet{widmer2002machine} shows that data-driven methods do learn some of the same rules as the KTH system but also learn rules that are the opposite of KTH rules. As has already been discussed, model evaluation's problematic nature may describe this phenomenon, as there is no telling which rule is more "correct" than another. Nevertheless, the KTH rule system has been an essential milestone in the evolution of EMP generation. 

\subsection{Data Based}\label{sec:data-based}
State of the art EMP generation models rely on existing human performance data to learn the mapping between score and performance. Such models are based either on sequential probabilistic or non-linear neural network methods\cite{cancino2018computational}, although there has been previous work with linear and non-sequential modeling. \citet{cancino2018computational} give a complete overview of all relevant EMP generation models. We will describe a few of these models and frameworks which are pertinent to our work

\subsubsection{Basis Function Models}
The first of these is a complete computational and mathematical framework for exploring EMP and is known as the Basis Model (BM) framework~\cite{eduardo2018computational}. The BM framework for EMP describes the full end-to-end process involving both the generation and analysis of musical performance. The name ``Basis Model'' is derived from the definition of Basis Functions, which create score features. The BM framework also defines \emph{expressive parameters}, which are analogous to our definition of performance features as outlined in section \ref{sec:performance}. Given score features defined by a set of basis functions and a set of expressive parameters that quantify a performance, the BM framework also provides models that can map the score features to expressive parameters\footnote{The BM framework is a mathematical outline of the components involved in EMP. The Basis Mixer is an open-source implementation of the BM framework and is made available on \href{https://github.com/OFAI/basismixer}{github}}. 

\citet{eduardo2018computational} outlines the full mathematical definition of the BM framework and the evolution of the framework and its application with specific feature and model definitions. BM models first started as simple linear non-sequential models that learned the relationship between a set of defined basis functions and a single expressive parameter, such as MIDI velocity. This version of the BM models each expressive parameter independently from all others and implies that one expressive parameter's interpretation will not affect the other. Although this is not necessarily the case in actual performance \footnote{For example, the effect of \mnot{crescendo} marking may have an affect on both the dynamics and the tempo at the same time}, it is a simplifying mathematical assumption that makes the models' development and interpretation simpler. Both standard least squares regression and a probabilistic Bayesian approach are used to model the linear relationship. 

The BM framework introduced both non-linear and sequential models, both in the form of ANNs, as its development progressed. A standard Feed-Forward Neural Network (FFNN) added the capability for non-linear modeling. This model showed an increase in both the goodness-of-fit and predictive accuracy over the linear model. A standard RNN was used to implement the sequential model with features where the time-dependent and sequential nature of music was relevant. Used in conjunction with a FFNN, this model performed the best relative to all other models.

\newcommand{\vnet}{VirtuosoNet}
\subsubsection{\vnet{}}
Similarly to the BM framework, the development of \vnet{} is gradual. The first version of the model presented by \citet{jeong2018virtuosonet} uses a recurrent hierarchical attention network (HAN) along with a novel \ed{} architecture specific to the EMP domain. This model introduces a global condition vector $c$, which can encode a specific style of performance. The decoder is autoregressive but also depends on the score notes as well as the global conditioning vector $c$. It models the generation of a performance note as $$p(y_t \vert y_1, ..., y_{t-1}, x, c)$$. This model's dataset is an early version of the KAIST dataset and consists of Chopin performances taken from the Piano-e-competition, with 25 compositions and 217 full performances. No quantitative or qualitative experiment results are given in the first formulation. 

The next iteration of \vnet{} uses a similar \ed{} architecture. However, it introduces an iterative sequential graph-based neural network (ISGN) that relies on the score representation as a graph data structure \cite{jeong2019graph}. The graph-based NN exists in conjunction with hierarchical attention layers in both the encoder and decoder. The latest and best-performing version of \vnet{}\cite{jeong2019virtuosonet} returns to the HAN only architecture. It adds additional more abstract hierarchical models that exist to create better structure at the metrical level and preserve patterns across mid-level structures of the composition. 

Both the ISGN\cite{jeong2019graph} and HAN\cite{jeong2019virtuosonet} version of \vnet{} are trained using the full KAIST dataset and the same evaluation methods; MSE for the quantitative evaluation and listening tests for the qualitative. The final version of HAN reports better MSE metrics than ISGN. The qualitative evaluation shows that both ISGN and HAN perform better than baseline models and better than a "deadpan" performance\footnote{A deadpan performance is one generated using the rule-based systems in most musical annotation software. The "deadpan" performance is one that sounds robotic, mundane, and without much musical expression} \rtodo[,inline]{An introduction of deadpan performances might work better in qualitative evaluation background}. The final HAN version's qualitative evaluation includes a comparison between the HAN and the publicly available version of the BM framework model \footnote{The website for the BM model can be found \href{here}{https://basismixer.cp.jku.at/static/app.html}. At the time of this writing, the website is currently unavailable}. 

The results in \cite{jeong2019virtuosonet} show that the HAN performs better than the BM model. There are many plausible reasons that may explain the difference in results other than the HAN being a superior model to the BM, including differences in the training data for both models, bias of the qualitative method towards the HAN, and the fact that the opinion of the members of the listening test doesn't necessarily imply one model being "superior" to another. However, given the results presented by \citet{jeong2019virtuosonet}, we will assume that this version of the HAN represents the current "state of the art" in the field, if such a thing even exists. 

\rtodo[,inline]{Add section that talks about the features used for virtuosoNet}

\section{Music Generation with Transformers}
The Transformer has seen some application in music generation. Such models use a discrete representation of music as a sequence of events with highly sparse encoding vectors. This representation is consistent with the word embeddings used to train most NLP models and so has a natural extension in Transformers, designed with language data in mind. 

The first model to use the Transformer for music generation is known as the "Music Transformer"~\cite{huang2018music}. It directly builds from the PerformanceRNN of ~\citet{oore2020time}. The PerformanceRNN uses the Piano-e-compeitition data to directly generate MIDI. This model \emph{simultaenously generates a composition and a performance}. The MIDI format encodes both aspects of a score such as the timing and pitch of notes but also elements of expressive performance such as dynamics and pedaling (see appendix \rtodo{add ref}). PerformanceRNN uses an event based representation of MIDI and all input tokens are a one hot encoding over the different possible MIDI events. There are 128 possible NOTE-ON events, 128 possible NOTE-OFF events, 125 possible TIME-SHIFT events, and 32 possible VELOCITY events, which leads to a 413 dimesional one hot encoded vector. PerformanceRNN is an autoregressive LSTM model which models the probably of generating the next note given all of the previous notes, similar to a language model. 

Music Transformer uses the original Transformer architecture and implementation of \citet{vaswani2017attention} with some adaptation, and applies it to the same data set and representation of the PerformanceRNN. One of the important parts of the original Transformer architecture is a positional embedding. This embedding encodes the sequential nature of the data into the model which is otherwise not accounted for using only the attention mechanism. The original positional embedding is referred to as an "absolute" position, meaning that the embedding only encodes information about the position of each element in relationship to the beginning of the sequence. Music Transformer uses a "relative" position, which instead adds information about how far apart two positions in the sequence are. The relative positional embedding accounts for every possible pairwise distance of each element and this information is then processed by the attention mechanisms throughout the rest of the model. It allows attention to account for the distance of any two notes to each other, rather than the global position of a single note in the sequence. 

The Transformer architecture with the relative positional embedding outperforms other baseline models, including the LSTM based PerformanceRNN. Qualitative evaluation shows that the Music Transformer generates performance with much better long term structure and overall musical cohesiveness than the performances from the PerformanceRNN\footnote{Sample performances for both the \href{https://magenta.tensorflow.org/performance-rnn}{PerformanceRNN} and \href{https://magenta.tensorflow.org/music-transformer}{Music Transformer} are  available online from the Magenta Research group of Google AI.}. This result is consistent with the observation that attention has a better memory than the hidden state of an RNN cell. 

Building from the Music Transformer, Open AI introduced MuseNet~\cite{payne_2020}. MuseNet is similar to GPT both in terms of the model architecture and philoshopy. It uses a very large decoder only Transformer model and trains it on a massive amount of data to build a model that is capable of generating novel music across many different domains. MuseNet is trained using only MIDI data gathered from various sources of the internet (including the MAESTRO dataset) and uses an event based token represenation similar to the Music Transformer. In contrast to the Music Transformer which only operates wester classical solo piano music, MuseNet is capable of generating music across a variety of genres in multiple instruments. Open AI has not directly released any research results comparing the benchmark results of MuseNet to other models, but it is evident from listening to MuseNet samples\footnote{\url{https://openai.com/blog/musenet/}} that it generates high quality music. 

Open AI also released music generation model which uses Transformers, JukeBox~\cite{dhariwal2020jukebox}, that deals directly in the audio domain. The choice to model directly in audio is motivated by the fact that symbolic forms such as MIDI and MusicXML do not capture the subleties of musical performance that are crucial in the actual experience of a musical listener. JukeBox is comprised of a Vector Quantized Variational AutoEncoder (VQ-VAE) whic learns to encode the audio data into a lower dimensional representation, and a Transformer model similar to MuseNet which uses the encoding to generate new music. Although the samples generated by JukeBox are impressive both for their sound quality and overall musical cohesiveness, much work is still needed in the direct audio generation domain to produce audio of high enough quality that it competes with human productions. 



