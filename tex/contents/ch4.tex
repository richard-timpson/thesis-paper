\chapter{Related Work: Music Generation Models}\label{ch:ch4}
As in other fields, state of the art models in music generation are mostly comprised of ANN models. Some music generation tasks have already seen a Transformer application, while others (such as expressive performance generation) have not. We will provide an overview of the existing state of the art in EMP generation and transformer-based music generation models. We also give some background into the data representation and feature engineering of each model. 

\section{Existing Expressive Musical Performance Generation Models}\label{sec:emp_generation_models}
EMP generation models fit into one of two categories, rule-based and data-based. Rule-based systems use hardcoded rules derived using pre-existing musical knowledge and empirical studies involving human cognition. Data-driven models rely on probabilistic and machine learning methods to take an existing dataset of both scores and performances and use the performance data as a guide to learn the mapping between score features and performance features. Such models are based either on sequential probabilistic or non-linear neural network methods\cite{cancino2018computational}, although there has been previous work with linear and non-sequential modeling. \citet{cancino2018computational} give a complete overview of all relevant EMP generation models. We describe a few models, both rule and data based, which are pertinent to our work. 

\subsection{Rule-Based EMP Models}
The KTH system \cite{friberg2006overview} sits at the center of rule-based EMP models and lays the foundation for all EMP generation models. Development of the KTH started in the 1980s and has continued well into the 21st century. The KTH system's initial methodology defined rules relating to musical composition structure and how it affects a resulting performance. The first set of rules applied specifically to singing synthesis were later adapted to general musical performance. 

Since then, there have been two general methods in the continued development of the KTH rule system. The first is that of \emph{analysis-by-synthesis}, which involved using the rules to synthesize musical performances presented to human listeners (both professional and non-professional), gathering listening feedback, and then using this feedback to modify the rules where needed. The second was an \emph{analysis-by-measurement} method. This method uses direct computation to evaluate a computational generated performance by comparing it with an existing real performance \footnote{This evaluation method is consistent with the data-driven approaches but generally applies to any generative model. Data-driven models use the performance data to directly build the model, whereas real performance data in the KTH system is for evaluation purposes only. Any further updates to the model still rely on a hardcoded set of rules}. Example rules from the KTH system are found in figure \ref{fig:kth-rules} \rtodo{May need more exploration in caption}. 

\begin{figure}
    \centering
    \missingfigure{show some of the rules from the KTH paper in either a table or a figure}
    \caption{The left column shows the name of the rule, and the right column provides a language description of that rule. These are the rules that we might expect a data-based system to learn.}
    \label{fig:kth-rules}
\end{figure}

To our knowledge, the KTH rule-based system is the first sophisticated computational model for generating expressive performance. The explicitly defined rules in the KTH system may be those we expect a data-based model to learn. \citet{widmer2002machine} shows that data-driven methods do learn some of the same rules as the KTH system but also learn rules that are the opposite of KTH rules. As has already been discussed, model evaluation's problematic nature may describe this phenomenon, as there is no telling which rule is more "correct" than another. Nevertheless, the KTH rule system has been an essential milestone in the evolution of EMP generation. 

\subsection{Data-Based EMP Models: Basis Function Models}\label{sec:data-based}
The Basis Function Modeling (BM) framework for EMP describes the full end-to-end process involving both the generation and analysis of musical performance. This end-to-end process includes mathematical definitions of both score and performance features, computational models for EMP, and the data processing necessary to convert MusicxML to input score features, and output performance features to MIDI. \citet{eduardo2018computational} outlines the full mathematical definition of the BM framework, the evolution of the framework, and its application with specific feature and model definitions.

\subsubsection{Features}
The name ``Basis Model'' is derived from the BM frameworks definition of ``Basis Functions''. Basis Functions capture different aspects of a score into numerical encodings (i.e. score features). There are some Basis Functions which are relatively straightforward, such as the duration of notes and MIDI pitch (we described these features as the ``low level'' of the score hierarchy in section \ref{sec:scores}), but most of the information in a score cannot be represented so easily. It becomes a theory-laden activity to use musical background knowledge to find good encodings of score information at the mid and high level.

% mp for matched performance
\newcommand{\mperf}{\beta_{\Omega}}

The BM framework defines $\Omega$ as a space containing aspects of a score. A score basis function $\varphi: \Omega \rightarrow \mathbb{R}$ which will map all elements of a score into some numerical expression based on an aspect of the score. The value of a score basis function for one element $x_i \in \Omega$ is denoted as $\varphi (x_i)$. For example, the score basis function for pitch, $\varphi_{pitch} (n_i) = \textrm{MIDI}_{\textrm{pitch}}(n_i)$ will represent the pitch of a given note as the numerical MIDI pitch. The basis function for dynamic markings, such as forte, would be defined as $\varphi_{\mnot{f}} (n_i) = forte(n_i)$, where $forte(n_i)$ returns 1 if the score element is marked with $\mnot{f}$, and 0 if it doesn't. 

Similarly to basis functions, the BM framework defines \emph{expressive parameters} which are mathematical encodings of information taken from a performance matched to a score, which is denoted as $\mperf$. An \emph{expressive encoding function} $y: \mperf \rightarrow \mathbb{R}$ will encode an event in a matched performance into an \emph{expressive parameter}. The value of each parameter corresponding to a score element $x_{i} \in \Omega$ is denoted as $y(x_i \vert \mperf) = t_i$ where $t_i \in \mathbb{R}$. Example encoding functions are $y_{\textrm{dyn}}(n_i \vert \mperf) = \textrm{MIDI}_{\textrm{vel}}(n_i)$ and $y_{\textrm{tempo}}(n_i \vert \mperf) = \textrm{bpm}(n_i)$, which represent expressive dynamics as MIDI velocity and expressive tempo as a local measure of bpm. 

With this definition of score basis fuctions and expressive encoding functions, the BM framework defines a score representation model $F = \{\varphi_1, ..., \varphi_m\}$ and a performance representation model $Y = \{y_1, ..., y_{N_y}\}$ which both contain a set of functions used to define the score and performance encoding. The score representation model can then be used to calculate the score features for every element $x_i \in \Omega$. This score representation $\Phi$ is a real valued matrix in $\mathbb{R}^{N \times M}$, where the i-th index indicates a single element of the score and the j-th index represents the value of a single score basis function for that score element. The performance representation model can render a performance representation $T$ which is calculated by running every score element along with a match performed through each expressive encoding function as $Y(x_i \vert \mperf) = (y_1(x_i \vert \mperf), ... ,y_{N_y}(x_i \vert \mperf))$. The performance representation $T$ is a real valued matrix in $\mathbb{R}^{N \times N_y}$ and composed of $N$ vectors defined by $Y(x_i \vert \mperf)$. 

There are many possible definitions for both basis and expressive encoding functions. Here we give some example functions used in the first iteration of the BM framework -- refer to \citet{eduardo2018computational} for a full list. 

\textbf{Basis Functions}

% pfunc for pitch function
\newcommand{\pfunc}[1]{\varphi_{\textrm{pitch}}^#1(n_i)}
\newcommand{\pfuncnorm}{\varphi_{\textrm{pitch}}(n_i)}
% midip for midi pithc
\newcommand{\midip}{\textrm{pitch}}
\begin{itemize}
    \item Polynomial Pitch Model: This model is presented by \citet{grachten2012linear} and describes the dependency of expressive dynamics (MIDI velocity) on pitch. This model defines three different pitch functions. $$\pfuncnorm{} = \frac{\midip (n_i)}{127} \quad \pfunc{2} = \left(\frac{\midip(n_i)}{127}\right)^2 \quad \pfunc{3} = \left(\frac{\midip (n_i)}{127}\right)^3$$
    \item Dynamics Markings: These are functions which encode dynamics markings such as \mnot{p}, \mnot{f}, and \mnot{crescendo}. There are three different types of dynamics encoding functions. The first is an impulse function  which is a binary indicator of a dynamic marking for a single note (this applies to single note marking such as an accent \wedge). The second is a ramp function which gradually increases or decreases a value from note to note (useful for \mnot{crescendo} or \mnot{decresencod} markings which apply to a specified number of notes\rtodo{add figure}). The last is a constant function which is either on or off for a set of notes at a time (useful for markings such as \mnot{p} or \mnot{f} which apply to a group of notes at a time). 
    \item Inter-Onset-Interval (IOI): This includes a total of six basis functions which include the difference in onset times of a group of notes belonging to a particular note onset $o_i$ an its surrounding note onsets. The IOI is calculated for the onsets between $(i - 2, i-3), (i-1, i-2), (i, i-1), (i, i+1), (i+1, i+2), \textrm{ and } (i+2, i+3)$. These functions provide context for the local tempo structure of the composition. 
    \item Duration: A basis function which encodes the notated duration of a single note. 
\end{itemize}

\textbf{Expressive Encoding Functions}

\newcommand{\iois}{\textrm{IOI}_{o(n_i)}^{score}}
\newcommand{\ioip}{\textrm{IOI}_{o(n_i)}^{perf}}
\begin{itemize}
    \item MIDI Velocity: This encoding function uses the recorded hammer-velocities of the performed notes as a proxy for expressive dynamics. The loudness of a note is estimated as a normalized MIDI velocity value. $$y_{vel}(n_i) = \frac{\textrm{vel}(n_i)}{127}$$
    \item log BPR: The Beat Period Ratio (BPR) is a way to define the local tempo calculated for every note. It uses the Beat Period (BP), defined as $$BP(n_i) = \frac{\ioip}{\iois}$$ where $\ioip$ and $\iois$ are the IOI for the performance and score onsets at note $n_i$. The final log BPR is defined as $$y_{\textrm{log bpr}}(n_i) = log_2\frac{\textrm{BP}(n_i)}{\textrm{BP}_{ave}}.$$ Here $\textrm{BP}_{ave}$ is the average BP over the entire piece. This normalizes the local BPR definition according the global tempo of the piece. 
    \item Timing (deviation): Whereas the log BPR defined above encodes the tempo of a performance (how ``fast'' it is being played), the function for timing encodes expressive timing deviations from the indicated timing of a score. It is defined as $$y_{\textrm{tim}} = \hat{o}^{perf}(n_i) - \textrm{onset}_{perf}(ni)$$ where $\hat{o}^{perf}(n_i)$ is an estimation of the ``correct'' onset time, and $\textrm{onset}_{perf}(ni)$ is the actual observed onset of a note (see section 3.3.1 of \citet{eduardo2018computational} for a full working example). 
    \item log Articulation: Articulation is a measure of the lengthening or shortening of a performed note duration relative to a noted duration and the current local tempo. The articulation is computed by dividing the measured note duration by the duration given by the score. It is defined as $$y_{\textrm{log art}}(n_i) = \textrm{log}_2\frac{\textrm{duration}_{perf}(n_i)}{\textrm{duration}(n_i)BP(n_i)}$$
\end{itemize}

\subsubsection{Models}
BM computation models first started as simple linear non-sequential models that learned the relationship between a set of defined basis functions and a single expressive parameter, such as MIDI velocity. This version of the BM models each expressive parameter independently from all others and implies that one expressive parameter's interpretation will not affect the other. Although this is not necessarily the case in actual performance \footnote{For example, the effect of \mnot{crescendo} marking may have an affect on both the dynamics and the tempo at the same time}, it is a simplifying mathematical assumption that makes the models' development and interpretation simpler. Both standard least squares regression and a probabilistic Bayesian approach are used to model the linear relationship. 

The BM framework introduced both non-linear and sequential models, both in the form of ANNs, as its development progressed. A standard Feed-Forward Neural Network (FFNN) added the capability for non-linear modeling. This model showed an increase in both the goodness-of-fit and predictive accuracy over the linear model. A standard RNN was used to implement the sequential model with features where the time-dependent and sequential nature of music was relevant. Used in conjunction with a FFNN, this model performed the best relative to all other models.

\newcommand{\vnet}{VirtuosoNet}
\newcommand{\vnetf}{VirtuosoNet framework}

\subsection{Data-Based EMP Models: \vnet{}}
\vnet{}, similarly to the BM framework, is a complete end-to-end system for training EMP models and generating novel performance. Although \vnet{} does not explicitly define a clear mathematical framework and abstract representations of EMP as does the BM framework, it nevertheless contains all of the moving parts necessary to train a EMP model from scratch. We make a distinction between \vnet{} as a set of NN-based EMP models and the \vnetf, which comes with all of the necessary data processing and featuring engineering as well as model training. 

\subsubsection{Features}
The \vnetf{} defines both input and output features. Input features contain standard score data such as pitch, timing, key, and metric information. The \vnetf{} uses these features as well as more detailed note level information such as the duration of rests, articulation markings, and others. The framework's output features are similar to those of the BM defined above and include tempo, note onset deviation, MIDI velocity, and articulation. Additionally, the \vnetf{}, unlike any other EMP generation model, uses features to model the pedal directly. They include the pedal value at note onset, pedal value at note offset, minimum pedal value between note onset and note offset, and minimum pedal value between note offset and the next note's onset. A full list of data features is given by \citet{jeong2019score}. 

\subsubsection{Models}
Similarly to the BM framework, the development of \vnet{} models is gradual (although the general model architecture is the same). The model comprises three parts; a score encoder, a performance encoder, and a performance decoder. The score encoder learns a score representation $C$, which is a sequence with the same length as the input notes. The performance encoder's job is to learn the style of a particular performance. This style guides performance generation so that different performances are rendered given the same input, depending on the learned style. The performance decoder is a generative model which uses the score encoding $C$ and the style vector $z$ to generate the resulting performance. 

The first version of the model presented by \citet{jeong2018virtuosonet} is a more primitive model and forms the basis for later versions. It introduces a recurrent hierarchical attention network (HAN) made up of hierarchical LSTM layers used in conjunction with the attention mechanism. The HAN forms the building block for all of the different components of \vnet{}. This model's dataset is an early version of the KAIST dataset and consists of Chopin performances taken from the Piano-e-competition, with 25 compositions and 217 full performances. No quantitative or qualitative experiment results are given in the first formulation. 

The next iteration of \vnet{} builds from the initial HAN layers but also uses a novel graph-based data representation and model. This model is defined as an iterative sequential graph-based neural network (ISGN). The graph-based NN exists in conjunction with hierarchical attention layers in both the encoder and decoder. The score is represented as a graph where the nodes are the individual notes. The graph's edges define relationships between the notes; one example is an edge indicating two notes with the same onset (which implies they belong to the same chord), another indicating notes belonging to a slur. A gated graph neural network (GGNN) is used to model the note-level graph representation and is combined iteratively with a HAN used to model measure-level features. The latest and best-performing version of \vnet{}\cite{jeong2019virtuosonet} returns to the HAN only architecture. Similar to the first \vnet{} model, it uses an LSTM based HAN network for every different component of the architecture, explicitly defining the hierarchical models according to musical boundaries, including notes, beats, and measures. This model adds additional more abstract hierarchical models that exist to create better structure at the metrical level and preserve patterns across mid-level structures of the composition.

Both the ISGN\cite{jeong2019graph} and HAN\cite{jeong2019virtuosonet} version of \vnet{} are trained using the full KAIST dataset and the same evaluation methods; MSE for the quantitative evaluation and listening tests for the qualitative. The final version of HAN reports better MSE metrics than the ISGN. The qualitative evaluation shows that both the ISGN and HAN perform better than baseline models and better than a "deadpan" performance\footnote{A deadpan performance is one generated using the rule-based systems in most musical annotation software. The "deadpan" performance is one that sounds robotic, mundane, and without much musical expression} \rtodo[,inline]{An introduction of deadpan performances might work better in qualitative evaluation background}. The final HAN version's qualitative evaluation includes a comparison between the HAN and the publicly available version of the BM framework model \footnote{The website for the BM model can be found \href{here}{https://basismixer.cp.jku.at/static/app.html}. At the time of this writing, the website is currently unavailable}. 

The results in \cite{jeong2019virtuosonet} show that the HAN performs better than the BM model. Many plausible reasons may explain the difference in results other than the HAN being a superior model to the BM. These include differences in the training data for both models, a bias of the qualitative method towards the HAN, and the fact that the listening test members' opinion doesn't necessarily imply one model being "superior" to another. However, given the results presented by \citet{jeong2019virtuosonet}, we will assume that this version of the HAN represents the current "state of the art" in the field (or as close to it as is possible). 

\rtodo[,inline]{Add section that talks about the features used for virtuosoNet}

\section{Music Generation with Transformers}
The Transformer has seen some application in music generation. Such models use a discrete representation of music as a sequence of events with highly sparse encoding vectors. This representation is consistent with the word embeddings used to train most NLP models and so has a natural extension in Transformers, designed with language data in mind. 

The first model to use the Transformer for music generation is known as the "Music Transformer"~\cite{huang2018music}. It directly builds from the PerformanceRNN of ~\citet{oore2020time}. PerformanceRNN uses the Piano-e-competition data to generate MIDI directly. This model \emph{simultaenously generates a composition and a performance}, combining the role of the composer and performer as presented in figure \ref{fig:generation_process} \rtodo{possibly add figure}. PerformanceRNN uses an event-based representation of MIDI, and all input tokens are a one-hot encoding over the different possible MIDI events. There are 128 possible NOTE-ON events, 128 possible NOTE-OFF events, 125 possible TIME-SHIFT events, and 32 possible VELOCITY events, which leads to a 413-dimensional one-hot encoded vector. PerformanceRNN is an autoregressive LSTM model which models the probability of generating the next note given all of the previous notes, similar to a language model. 

Music Transformer uses the original Transformer architecture and implementation of \citet{vaswani2017attention} with some adaptation and applies it to the same data set and representation of the PerformanceRNN. One crucial component of the original Transformer architecture not mentioned is its positional embedding. This embedding encodes the sequential nature of the data into the model, which is otherwise not accounted for using only the attention mechanism. The original positional embedding is an "absolute" position, meaning that the embedding only encodes information about each element's position in relation to the beginning of the sequence. Music Transformer uses a "relative" position, which instead adds information about how far apart two positions in the sequence are. The relative positional embedding accounts for every possible pairwise distance of each element. The attention mechanism processes this information throughout the rest of the model and allows attention to account for the distance of any two notes to each other, rather than the global position of a single note in the sequence. 

The Transformer architecture with the relative positional embedding outperforms other baseline models, including the LSTM based PerformanceRNN. Qualitative evaluation shows that the Music Transformer generates performance with much better long term structure and overall musical cohesiveness than the performances from the PerformanceRNN\footnote{Sample performances for both the \href{https://magenta.tensorflow.org/performance-rnn}{PerformanceRNN} and \href{https://magenta.tensorflow.org/music-transformer}{Music Transformer} are  available online from the Magenta Research group of Google AI.}. This result is consistent with the observation that attention has a better memory than the hidden state of an RNN cell. 

Building from the Music Transformer, Open AI introduced MuseNet~\cite{payne_2020}. MuseNet is both philosophically and architecturally similar to GPT. It uses an enormous decoder-only Transformer model and trains it on a massive amount of data to build a model capable of generating novel music across many different domains. MuseNet is trained using only MIDI data gathered from various internet sources (including the MAESTRO dataset) and uses an event-based token representation similar to the Music Transformer. In contrast to the Music Transformer, which only operates with western classical solo piano music, MuseNet can generate music across a variety of genres in multiple instruments. Open AI has not directly released any research results comparing the benchmark results of MuseNet to other models. Still, it is evident from listening to MuseNet samples\footnote{\url{https://openai.com/blog/musenet/}} that it generates high-quality music. 

Open AI also released another music generation model which uses Transformers, JukeBox~\cite{dhariwal2020jukebox}, that deals directly in the audio domain. The choice to directly model audio is motivated by the fact that symbolic data forms don't contain enough information to capture musical performance's subtleties across a variety of different instruments% 
\footnote{It is also motivated by the abundance of audio data compared to symbolic, which we discussed in chapter \ref{ch:ch2}}. JukeBox is comprised of a Vector Quantized Variational AutoEncoder (VQ-VAE), which learns to encode the audio data into a low-dimensional representation, and a Transformer model similar to MuseNet, which uses the encoding to generate new music. Although the samples generated by JukeBox are impressive in their sound quality and overall musical cohesiveness, much work is still needed in the direct audio generation domain to produce audio of high enough quality that it competes with human productions. 



