\chapter{Methods and Experiments}\label{ch:ch4}
Given the relevant background research and knowledge base, we will now describe the experiments we ran and the reason behind our experimental methods. Given the powerful advances in NLP due to the Transformer discussed in \ref{sec:transformers}, our general goal was to investigate the results of the Transformer in application to EMP generation, which to our knowledge has never been done. Because both language and music are highly sequential and hierarchical in nature, our intuition was that because the Transformer does a good job of learning the general structure of language, that it can do the same of for music. We use the general framework for a complete end to end performance generation system which is proposed by virtuosoNet. In its simplicity, the initial purpose of this project was to determine if a Transformer based model can improve upon virtuosoNet, given the same data, features, and evaluation metrics\rtodo[,inline]{Make sure to add a section about feature engineering with virtuosoNet}. However, due to the highly ambiguous and subjective nature of EMP generation, there was no clear way to know if we confidently answer this question given our results. As such, we modified our research direction to providing additional insight and intuition about the nature of EMP generation itself and how this intuition can guide future work. 

\section{Data and Features}
The reasons for the adoption of the virtuosoNet system are twofold: the first being that the dataset used to develop virtuosoNet was the largest publicly available dataset used in EMP generation, and the second being that the code and models of virtuosoNet are open-sourced \footnote{https://github.com/jdasam/virtuosoNet} and contain all of the necessary data processing. This system also somewhat represents the "state of the art" in EMP generation, so it provides a natural starting place to use for comparison against any further model development. The virtusoNet system uses handcrafted features for both scores and performances. Score features contain low-level information (pitch and timing), high-level information such as the key and metric information, as well as more detailed information such as the duration of rests, articulation markings (legato and staccato), and the distance from the closest preceding tempo and dynamics directions, slur, and beam status. The performance features include all of the standard performance features: tempo expressed as BPM, note onset deviation, MIDI velocity, articulation, and different features related to the onset and offset times of the pedal. A full outline of the features is given in \cite{jeong2019score}. 

\section{Model}
In the virtuosoNet system, there is a 1:1 mapping between notes in scores and performances. The original Transformer as an encoder-decoder model was designed as a seq-2-seq model where the sequences have different lengths, which adds additional complexities into the model to account for this difference. To keep our system simple, our model is conceptually similar to BERT, and acts as an encoder-only Transformer model.  It contains a simple fully connected linear layer on top which will learn the final mapping between the Transformer encoding and the actual score features. We use the standard absolute positional embedding which is concatenated with the score features as input to the model. The performance output features of the model can be used to construct a MIDI file, allowing for the system to performance full performance generation given a score in MusicXML form. 

\section{Experiments and Model Evaluation}\label{sec:experiments-and-evaluation}
virtuosoNet is built as a regression model and uses MSE as both its loss function and evaluation metric. It uses an 8-1-1 train/valid/test data split, and \citet{jeong2019virtuosonet} present MSE results for each different parameter of the performance features on the test set. We follow the same method for our quantitative evaluation. Most models were trained at 50 epochs, and the best model parameters were selected according to the lowest validation evaluation score. We used the software Neptune AI \cite{neptune} to manage our experiments and report the metric feedback. Data for all of the experiments we ran including the model hyperparameters and metrics can be found online\footnote{\url{http://ui.neptune.ai/richt3211/thesis}}. 

We ran experiments using the same data for several different model configurations. Similarly to our Transformer encoder model, we build an LSTM baseline model with 3 recurrent layers which acts as an encoder and a simple fully connected linear on top to perform the final mapping between the LSTM encoding and the output features. The LSTM baseline is 3 layers with a hidden size of 256, and is used for comparison purposes only. 

\subsection{Quantitative Evaluation}
We use several different model configurations for the Transformer. Our Transformer baseline has 6 layers, 6 attention heads, and a hidden size of 256. We chose this as a base configuration because it closely matches the size of the original Transformer \cite{vaswani2017attention}, except for the hidden size of the feed-forward layer. We chose a smaller hidden size of 256 for our base layer hidden size due the relatively small dimensionality, 78, of the input data to the model. Our initial goal was to find the optimal model configuration according to a quantitative evaluation, which meant finding the lowest total MSE loss. To keep our modeling honest, we withheld from running the final test evaluation until all models had been trained. As such, we needed a way to compare against existing virtuosoNet models without using the final MSE evaluation metrics reported by \citet{jeong2019virtuosonet}. To do this, we trained from scratch the Iterative Sequential Graph Network (ISGN) and the HAN baseline (HAN-BL) models as reported in \cite{jeong2019graph} and \cite{jeong2019virtuosonet} respectively, and used the metrics from the validation data set to guide our own model development before we ran the final evaluation. We ran a fairly exhaustive search of a single dimension of the hyperparameters at a time. The full results of these models are show in table \ref{tab:quantitative} \rtodo{add table reference}. 

\subsection{Qualitative Evaluation: Identifying Training Problems}\label{sec:qualitative-eval-problems}
During our model development we conducted our own listening tests to subjectively determine the quality of each model. It was apparent from the start that there was a mismatch between the quantitative results of the model and it's quality in a listening test. For example, the Transformer model with $N_{id}$ 125 (which we will denote as $T_{N_{125}}$, see Table \ref{tab:quantitative}) had much worse validation MSE metrics than almost every other model Transformer model. However, a listening test revealed that the absolute tempo for smaller models, such as the Transformer baseline $T_{N_{147}}$, was much faster and sounded worse (to the point where the performances are almost 'unlistenable') than $T_{N_{125}}$, even though it presented better quantitative metrics on the validation test set. The potential disconnect between the 'quality' of the model as determined by quantitative and qualitative evaluation led us to investigate potential problems with the training methods used by \citet{jeong2019virtuosonet}. See section \rtodo{Add ref} for a more in-depth discussion of this evaluation. 

One of the first potential problems we identified was the method used to calculate and interpret the loss and evaluation. The output features of virtuosoNet are represented by a sequence of vectors with a length of 11. The first 4 features are values that correspond to a single expressive parameter, and are tempo, velocity, deviation, and articulation, respectively. The last 7 features are all different numbers that correspond to information about the pedal \cite{jeong2019score}. \citet{jeong2019virtuosonet} present MSE metrics for five different expressive parameters, which include all of those previously mentioned, as well as the pedal. This means that when we refer to the pedal MSE, it is an aggregation of the 7 different features that contain pedal information. The original MSE which was used to train virtuosoNet assumed that every feature of the output vector contributed equally to the final output and corresponding loss optimization. Given that there is 7 times more information for the pedal parameter than all others, we can think of this loss function as placing much more importance for the pedal than every other expressive feature. To combat this, we came up with a new weighted MSE loss function that allows for the optimization of some features over another. 

\newcommand{\rvec}[1]{\mb{#1}}

% vn for vector normal. predicted
% vh for vector hat. target
\newcommand{\vn}{\rvec{v}}
\newcommand{\vh}{\rvec{\hat{v}}}

% df for difference
\newcommand{\df}[1]{(\vn_{#1} - \vh_{#1})^2}

% al for alpha
\newcommand{\al}[1]{\alpha_{#1}}

We define the output vector as an 11 dimensional vector $\rvec{v} = \{t, v, d, a, p_0, p_1, p_2, p_3, p_4, p_5, p_6\}$ where $t$, $v$, $d$, and $a$ represent tempo, velocity, deviation, and articulation respectively, and $p_i$ represents a single component of the pedal. For a predicted output vector $\vn$ and the target output vector $\vh$, standard MSE loss is $MSE(\vn, \vh) = \frac{1}{n}\sum_{i=1}^{n}(\vn_i - \vh_i)^2$. This can also be re-written as $MSE(\vn, \vh) = \frac{1}{11}[\df{t} + \df{v} + \df{d} + \df{a} + \sum_{i=1}^{7}(\vn_{p_{i}} - \vh_{p_{i}})^2]$. We introduce 5 different weight values: $\al{t}, \al{v}, \al{d}, \al{a}$ and $\al{p}$. Our weighted MSE loss is defined as $W_{MSE}(\vn, \vh) = \frac{1}{\al{t} + \al{v} + \al{d} + \al{a} + \al{p}}[\al{t}\df{t} + \al{v}\df{v} + \al{d}\df{d} + \al{a}\df{a} + \al{p}\sum_{i=1}^{7}(\vn_{p_{i}} - \vh_{p_{i}})^2 ]$. The original MSE can be seen as the weighted MSE with $\al{t}, \al{v}, \al{d}, \al{a} = 1$, and $\al{p} = 7$. If we conceptualize the loss optimization in this way, we can view the original model optimization as placing much more importance towards accuracy in the pedal than any other feature of expression. The MSE was used not only as the loss function to optimize the model, but also as the actual metric to evaluate the model with. This evaluation means that models with an emphasis in pedal accuracy will be preferred over those without it. This presents the question of determining whether or not this is the right way to conceptualize a 'good' model. Would a different configuration of the expressive feature weights lead to a better outcome? These answers are non-trivial and this further emphasizes the importance of having better ways to both optimize and evaluate EMP generation models. 

We also changed the way in which the loss was calculated for the articulation feature. As discussed in section \ref{sec:data-based}, virtuosoNet uses an alignment algorithm which presents metadata about the alignment between the score and performance of every single note. The notes that are not aligned are included in the input data to the model, but are excluded from the loss calculation. A similar method is used with notes relating to the articulation feature and the pedal. \citet{jeong2019virtuosonet} say \say{Since the articulation is largely affected by the sustain pedal, we reduced the weight for the articulation loss to 0.1 for notes with the sustain pedal pressed at the offset}. In the actual data generation code the weights for the articulation loss calculation are slightly more complicated than what is presented in the paper \footnote{See \href{https://github.com/jdasam/pyScoreParser/blob/1d36e6f4f46d835f3ad08fea66892f475ac08dd9/xml_matching.py\#L323}{github}}, but the intuition behind changing the loss for notes used in combination with pedal is the same. For some experiments we change this loss calculation for articulation to fall in line with all other performance features which involves using alignment data only to exclude notes from the loss. Again, this type of optimization and evaluation is subjective and it is hard to say if one is more correct than another. 

Changing the loss function (as well as the evaluation function) in such ways alters the interpretability of the metric and invalidates the direct comparison to the metrics reported for virtuosoNet. However, we can still compare model outputs qualitatively. Due to time and resource constraints, no sophisticated qualitative evaluation method was used to conduct this comparison. Our qualitative evaluation relied mostly on the authors own listening tests and internal discussions about the quality of the performances and potential places for error. The listening tests were conducted by comparing performances of 6 different compositions for each model both audibly and visually using the Digital Audio Workspace (DAW) software Logic Pro X. The performances are listend in table \rtodo{Add reference} No numerical or statistical observations are reported given the fact that all evaluation was done by the author and represents an inherent bias which and cannot be seen as robust or reliable for further analysis. We do however provide some observations related to our qualitative evaluation along with our own intepretation of them, if for no other reason than to guide the intuition behind more robust methods for future work. This analysis is given in section \rtodo{Add reference}.

\begin{table}
    \begin{center}
    \begin{tabular}[]{| c | c |}
        \hline
        Composition & Composer \\ 
        \hline 
        test & test \\
        test & test \\ 
        \hline
    \end{tabular}
    \end{center}
\end{table}

