\chapter{Related Work: Music Generation Models}\label{ch:ch4}
As in other fields, state of the art models in music generation are mostly comprised of ANN models. Some music generation tasks have already seen a Transformer application, while others (such as expressive performance generation) have not. We will provide an overview of the existing state of the art in EMP generation and transformer-based music generation models. We also give some background into the data representation and feature engineering of each model. 

\section{Existing Expressive Musical Performance Generation Models}
EMP generation models fit into one of two categories, rule-based and data-based. Rule-based systems use hardcoded rules derived using pre-existing musical knowledge and empirical studies involving human cognition. Data-driven models rely on probabilistic and machine learning methods to take an existing dataset of both scores and performances and use the performance data as a guide to learn the mapping between score features and performance features. 

\subsection{Rule Based}
The KTH system \cite{friberg2006overview} sits at the center of rule-based EMP models and lays the foundation for all EMP generation models. Development of the KTH started in the 1980s and has continued well into the 21st century. The KTH system's initial methodology defined rules relating to musical composition structure and how it affects a resulting performance. The first set of rules applied specifically to singing synthesis were later adapted to general musical performance. 

Since then, there have been two general methods in the continued development of the KTH rule system. The first is that of \emph{analysis-by-synthesis}, which involved using the rules to synthesize musical performances presented to human listeners (both professional and non-professional), gathering listening feedback, and then using this feedback to modify the rules where needed. The second was an \emph{analysis-by-measurement} method. This method uses direct computation to evaluate a computational generated performance by comparing it with an existing real performance \footnote{This evaluation method is consistent with the data-driven approaches but generally applies to any generative model. Data-driven models use the performance data to directly build the model, whereas real performance data in the KTH system is for evaluation purposes only. Any further updates to the model still rely on a hardcoded set of rules}. Example rules from the KTH system are found in figure \ref{fig:kth-rules} \rtodo{May need more exploration in caption}. 

\begin{figure}
    \centering
    \missingfigure{show some of the rules from the KTH paper in either a table or a figure}
    \caption{The left column shows the name of the rule, and the right column provides a language description of that rule. These are the rules that we might expect a data-based system to learn.}
    \label{fig:kth-rules}
\end{figure}

To our knowledge, the KTH rule-based system is the first sophisticated computational model for generating expressive performance. The explicitly defined rules in the KTH system may be those we expect a data-based model to learn. \citet{widmer2002machine} shows that data-driven methods do learn some of the same rules as the KTH system but also learn rules that are the opposite of KTH rules. As has already been discussed, model evaluation's problematic nature may describe this phenomenon, as there is no telling which rule is more "correct" than another. Nevertheless, the KTH rule system has been an essential milestone in the evolution of EMP generation. 

\subsection{Data Based}\label{sec:data-based}
State of the art EMP generation models rely on existing human performance data to learn the mapping between score and performance. Such models are based either on sequential probabilistic or non-linear neural network methods\cite{cancino2018computational}, although there has been previous work with linear and non-sequential modeling. \citet{cancino2018computational} give a complete overview of all relevant EMP generation models. We will describe a few of these models and frameworks which are pertinent to our work

\subsubsection{Basis Function Models}
The first of these is a complete computational and mathematical framework for exploring EMP and is known as the Basis Model (BM) framework~\cite{eduardo2018computational}. The BM framework for EMP describes the full end-to-end process involving both the generation and analysis of musical performance. The name ``Basis Model'' is derived from the definition of Basis Functions, which create score features. The BM framework also defines \emph{expressive parameters}, which are analogous to our definition of performance features as outlined in section \ref{sec:performance}. Given score features defined by a set of basis functions and a set of expressive parameters that quantify a performance, the BM framework also provides models that can map the score features to expressive parameters\footnote{The BM framework is a mathematical outline of the components involved in EMP. The Basis Mixer is an open-source implementation of the BM framework and is made available on \href{https://github.com/OFAI/basismixer}{github}}. 

\citet{eduardo2018computational} outlines the full mathematical definition of the BM framework and the evolution of the framework and its application with specific feature and model definitions. BM models first started as simple linear non-sequential models that learned the relationship between a set of defined basis functions and a single expressive parameter, such as MIDI velocity. This version of the BM models each expressive parameter independently from all others and implies that one expressive parameter's interpretation will not affect the other. Although this is not necessarily the case in actual performance \footnote{For example, the effect of \mnot{crescendo} marking may have an affect on both the dynamics and the tempo at the same time}, it is a simplifying mathematical assumption that makes the models' development and interpretation simpler. Both standard least squares regression and a probabilistic Bayesian approach are used to model the linear relationship. 

The BM framework introduced both non-linear and sequential models, both in the form of ANNs, as its development progressed. A standard Feed-Forward Neural Network (FFNN) added the capability for non-linear modeling. This model showed an increase in both the goodness-of-fit and predictive accuracy over the linear model. A standard RNN was used to implement the sequential model with features where the time-dependent and sequential nature of music was relevant. Used in conjunction with a FFNN, this model performed the best relative to all other models.

\newcommand{\vnet}{VirtuosoNet}
\subsubsection{\vnet{}}
Similarly to the BM framework, the development of \vnet{} is gradual. The first version of the model presented by \citet{jeong2018virtuosonet} uses a recurrent hierarchical attention network (HAN) along with a novel \ed{} architecture specific to the EMP domain. This model introduces a global condition vector $c$, which can encode a specific style of performance. The decoder is autoregressive but also depends on the score notes as well as the global conditioning vector $c$. It models the generation of a performance note as $$p(y_t \vert y_1, ..., y_{t-1}, x, c)$$. This model's dataset is an early version of the KAIST dataset and consists of Chopin performances taken from the Piano-e-competition, with 25 compositions and 217 full performances. No quantitative or qualitative experiment results are given in the first formulation. 

The next iteration of \vnet{} uses a similar \ed{} architecture. However, it introduces an iterative sequential graph-based neural network (ISGN) that relies on the score representation as a graph data structure \cite{jeong2019graph}. The graph-based NN exists in conjunction with hierarchical attention layers in both the encoder and decoder. The latest and best-performing version of \vnet{}\cite{jeong2019virtuosonet} returns to the HAN only architecture. It adds additional more abstract hierarchical models that exist to create better structure at the metrical level and preserve patterns across mid-level structures of the composition. 

Both the ISGN\cite{jeong2019graph} and HAN\cite{jeong2019virtuosonet} version of \vnet{} are trained using the full KAIST dataset and the same evaluation methods; MSE for the quantitative evaluation and listening tests for the qualitative. The final version of HAN reports better MSE metrics than ISGN. The qualitative evaluation shows that both ISGN and HAN perform better than baseline models and better than a "deadpan" performance\footenote{A deadpan performance is one generated using the rule-based systems in most musical annotation software. The "deadpan" performance is one that sounds robotic, mundane, and without much musical expression} \rtodo[,inline]{An introduction of deadpan performances might work better in qualitative evaluation background}. The final HAN version's qualitative evaluation includes a comparison between the HAN and the publicly available version of the BM framework model \footnote{The website for the BM model can be found \href{here}{https://basismixer.cp.jku.at/static/app.html}. At the time of this writing, the website is currently unavailable}. 

The results in \cite{jeong2019virtuosonet} show that the HAN performs better than the BM model. There are many plausible reasons that may explain the difference in results other than the HAN being a superior model to the BM, including differences in the training data for both models, bias of the qualitative method towards the HAN, and the fact that the opinion of the members of the listening test doesn't necessarily imply one model being "superior" to another. However, given the results presented by \citet{jeong2019virtuosonet}, we will assume that this version of the HAN represents the current "state of the art" in the field, if such a thing even exists. 

\rtodo[,inline]{Add section that talks about the features used for virtuosoNet}

\section{Music Generation with Transformers}
The Transformer has seen some application in music generation. Such models use a discrete representation of music as a sequence of events with highly sparse encoding vectors. This representation is consitent with the word embeddings used to train most NLP models, and so has a natural extension in Transformers which are designed with language data in mind. 

The first model to use the Transformer for music generation is known simply as the "Music Transformer"~\cite{huang2018music}. This model uses two data sources. One is the J.S Bach Corales dataset which has canonically been used for generative music models. The other is the Piano-e-competition 
