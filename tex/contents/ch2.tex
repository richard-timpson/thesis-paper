\chapter{Background}\label{ch:ch2}
Provide additional context to the problem of expressive musical performance (EMP) and the model domain we are applying (Transformers). Make sure to talk about the intricacies behind the musical side of the problem, given that it is generally not as well known in computer science, ML, and AI. Introduce the idea of music information retrieval (MIR) research, and how EMP fit's into this research. Provide sufficient detail at a high level detailing exactly what EMP is and why it is an interesting problem, specifically for machine learning. Cover what type of data is required for the problem.

Cover the Transformer and why it is worth it to apply this model to the problem domain. 

\section{Expressive Performance Generation}
Define expressive performance generation (EPG) at a technical level (data features). Give background into how it fits into MIR research. 
\begin{itemize}
    \item Define a Score and Performance
    \begin{itemize}
        \item Talk about differences between score and performance at a higher level.
        \item Score includes symbolic representation of music and includes pitch, tempo (sometimes), timing, dynamics, and phrasing. 
        \item Performance is an interpretation of a score. Includes the note pitches, tempo, timing, deviation, articulation, and dynamics. 
        \item EPG is the task of creating a model which takes in a score (usually in the form of MusicXML) and outputs a performance (usually MIDI). \rtodo{Add reference to data format section}
        \item Score to Performance Alignment. Necessary to the notes of a performance with their corresponding position in a score. Because performances are so varied, this is a non-trivial problem. 
        \item Papers 
        \begin{itemize}
            \item Basis Mixer \cite{eduardo2018computational}
            \item Computational Models for Expressiveness \cite{cancino2018computational}
        \end{itemize}
        \item \rtodo[,inline]{Add reference to later section which talks about feature engineering in detail}
        \item \rtodo[,inline]{Create (find) figures for score and performance}
    \end{itemize}
    \item Explain how expressive performance generation fits into music generation research
    \begin{itemize}
        \item Generation as subset of MIR research 
        \item Different components of generation. Composition, performance, and synthesis. 
        \item \rtodo[,inline]{Create graph showing (or referencing other graphs) of where performance generation fits into music generation as a whole}
        \item Papers 
        \begin{itemize}
            \item This time with feeling \cite{oore2020time}
            \item Deep learning for music generation survey \cite{ji2020comprehensive}
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Data}
A brief section about the data used for the problem. Introduce MusicXML and MIDI
\begin{itemize}
    \item MusicXML
    \begin{itemize}
        \item A text based representation of a musical score. 
        \item Created as a way to standardize score data among different notation software. 
        \item Useful for EMP research because of the standardized format. 
        \item Contains all relevant information about the score and it's related features. \rtodo{Add reference to feature section}
    \end{itemize}
    \item MIDI 
    \begin{itemize}
        \item Event based protocol for digital representation of musical instruments. 
        \item Used in a variety of ways, most commonly known for it's use in DAW software to represent easily editable tracks for music production. 
        \item Can be synthesized in many different ways. 
        \item Contains all of the needed information to represent a musical performace. \rtodo{Reference feature section}. 
    \end{itemize}
    
\end{itemize}



\section{Transformers}
Provide context to why transformers are important and the problems they've solved in nlp. 
\begin{itemize}
    \item Intuition behind transformers and why they are so powerful in sequence modeling
    \item Attention is all you need paper \cite{vaswani2017attention}
    \begin{itemize}
        \item State of the art in translation tasks
        \item New architecture for sequence modeling using only attention. No recurrent network
    \end{itemize}
    \item BERT \cite{devlin2018bert}
    \begin{itemize}
        \item Transformer Encoder only 
        \item Self-supervised learning and pre-training. Includes having a simple multi-layer perceptron at the end to make it useful
    \end{itemize}
    \item Music Transformer \cite{huang2018music}
    \begin{itemize}
        \item Builds off of This Time with Feeling\cite{oore2020time} paper. Both composition and performance generation at the same time 
        \item Implements full transformer architecture 
        \item Achieves better results than LSTM
    \end{itemize}
    \item Question: Can a transformer model be applied to only performance generation with an encoder only architecture to achieve better results than current state of the art models?. Intuition says yes given the results from Music Transformer. 
\end{itemize}



\section{Evaluation}
\begin{itemize}
    \item Evaluation is particularly difficult for a problem like EPG because there is no "correct" interpretation of a score. However, there is at least a vaguely understood relationship between a score marking and how a performaner should use that marking within the context of a performance. For example, if a crescendo marking is used in a score, the performer should at the very least increase the volume of the performance relative to the current volume of the piece. The amount which the volume should increase or the rate at which it increases are not clearly defined, but the fact of the increase of volume itself is. This is the fundamental intuition behind the motivation to build computational models for expressive performance. Nonetheless, it still remains a difficult job to evaluate a given EPG model because of the ambiguity of what is "correct" or not.
    \item Evaluation methods used so far in EPG models are broken into two categories, quantitative and qualitative. 
    \item Quantitative: 
    \begin{itemize}
        \item This follows standard techniques for experimentation of evaluation of ML models in general. It usually involves calculating a numerical value for a models inference on a separate test data set that was not used for model training or model selection. \rtodo{Find reference for ML training and evaluation}. Common metrics for regression like problems are mean squared error (MSE) and the pearson correlation coefficient (R2). 
        \item Due to the nature of EPG model evaluation mentioned above, it is not clear that "better" quantiative metric score for a given model over another indicates that the performance of the model is superior. \rtodo{Find section in Garcon survey that references this point}. 
    \end{itemize}
    \item Qualitative
    \begin{itemize}
        \item Qualitative evaluation methods involve gathering human feedback by playing performances of a given models performance to an audience and getting ratings or judgement of the model according to a predefined questionnare or survey method. The nature of these evaluation methods is not consistent in the current literature and remains a challenge for the field to solve in the future. \rtodo{Find section in Garcon survey that references this point}. 
        \item \rtodo[, inline]{Conduct more research for reference on current methods for qualititative evaluation}
    \end{itemize}
\end{itemize}


% Here shows to insert figures and cite figures in the main text.

% \begin{figure}[h!]
% \centering
% % \includegraphics[width = 0.85\linewidth]{./figs/ch2/lena.bmp}
% \caption{Picture of Lena}
% \label{fig:2-fig1}
% \end{figure}
% Picture of lena is shown in Fig. \ref{fig:2-fig1}.
