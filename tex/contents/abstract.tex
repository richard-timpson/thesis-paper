Current state-of-the-art modeling of expressive musical performance (EMP) is based on hierarchical Recurrent Neural Networks (RNN). The Transformer is a recent Neural Network (NN) sequence modeling architecture that has led to significant research improvement in Natural Language Processing (NLP) and other related fields. To date, there has been no application of the Transformer in music performance modeling. We present the first study that attempts to do so. The results indicate that our encoder-only Transformer model outperforms a similar encoder-only RNN but does not outperform the existing hierarchical RNN state-of-the-art. However, an analysis of current evaluation methods reveals for EMP generation models shows that the current metrics for "objective" quantitative evaluation do not correctly capture the essence of musical performance and are significant bottlenecks in developing robust EMP models. We present an error analysis of the current evaluation methods and provide suggestions for future efforts to build better models and find better evaluation metrics