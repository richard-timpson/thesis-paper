\chapter{Results} \label{ch:ch5}

\section{Quantitative Evaluation Results}
As discussed in section \ref{sec:experiments-and-evaluation}, the original purpose of this project was to determine if a Transformer based model could outperform the existing LSTM virtuosoNet models. Table \ref{tab:quantitative} shows the results of our experiments in comparison with the virtuosoNet models. The MSE metrics used for comparison with the virtuosoNet models are taken from \cite{jeong2019virtuosonet}. We also present the same performance metrics for our own LSTM baseline model as an additional comparison. All of the models presented in this table are trained using the standard MSE without weighted expressive parameters and use the articulation MSE calculation according to the pedal status as discussed in \ref{sec:qualitative-eval-problems}. To the best of our effort, all models were trained and evaluated using the same data, features, and evaluation metric. 

\rtodo[,inline]{Explain the results in the table when all experiments are done running}. 

% using macros in case the values change. 

% model configuration
\newcommand{\nep}{$N_{id}$}
\newcommand{\mn}{$M$} % mn for 'model name'
\newcommand{\nl}{$L$} % nl: num layers
\newcommand{\dhid}{$d_{hid}$} % dhid: dimension hidden size
\newcommand{\drop}{$D$} % D: Dropout
\newcommand{\lr}{$LR$} % LR: Learning Rate
\newcommand{\clip}{$C$} % C: gradient clip
\newcommand{\nh}{$H$} % nh: num heads

% Expressive features
\newcommand{\temp}{$t$}
\newcommand{\vel}{$v$}
\newcommand{\dev}{$d$}
\newcommand{\art}{$a$}
\newcommand{\ped}{$p$}


% my results
                   %total              %tempo              %vel              %dev              %articul              %pedal        
\readlist*\a{1.079915467915403, 0.8387099791654173, 1.3530433499396628, 1.017870266429722, 1.1067559519102133, 1.080384319222477} % 123 
\readlist*\b{0.8634560842141723, 0.5412064723018719, 0.8022146236503704, 0.8804639799541721, 0.8022366483458304, 0.9245564142364828} % 147 
\readlist*\c{0.8714990162701071, 0.5505609703737273, 0.7881732323421042, 0.9611995326543272, 0.8751485813981792, 0.9159152227103448} % 169 
\readlist*\d{0.8608967689267041, 0.5025553895868159, 0.7638317284792879, 0.878050578836977, 0.8178488211257614, 0.929653946913942} % 128 
\readlist*\e{0.8269241870319757, 0.4671040222800673, 0.7613456284477763, 0.8818908914978052, 0.8233950348666115, 0.8803471737534461} % 133 
\readlist*\f{0.892434075736163, 0.6518531192930515, 0.8248660148758638, 0.8837178772102976, 0.822714017388242, 0.9476605038763138} % 118 
\readlist*\g{0.8398954164095592, 0.5103522019637757, 0.7708808816209132, 0.9533001596417616, 0.8093733305095236, 0.8849918198915682} % 132 
\readlist*\h{0.9055077961121484, 0.7380731321188851, 0.8207453556958286, 0.9515869074832948, 0.8602034320048559, 0.9414252330302848} % 171 
\readlist*\i{1.005084838962903, 0.8624353822994071, 1.0529326007749042, 0.8987197173990472, 1.1763732810960201, 1.0093531322087683} % 173 
\readlist*\j{0.8463679141699834, 0.5445592460407571, 0.7492400507566016, 0.899280836268869, 0.8597047927381344, 0.8938945523819722} % 134 
\readlist*\k{0.8436574470387758, 0.4748660706616556, 0.8106850048921405, 0.8946506051604983, 0.8585572391965963, 0.8916389604554559} % 135 
\readlist*\l{0.9331073568122104, 0.6931559999619044, 0.9870281539322866, 0.9139575008493828, 1.1235607440091158, 0.9352111728264682} % 125 

% virtuosoNet results. Copied from other papers

                        % total      %temp, %vel, %dev, %art, %pedal  
\readlist*\vbl{0.7698181818181818, 0.4, 0.673, 0.773, 0.721, 0.843} 
\readlist*\vs{0.7324545454545454, 0.269, 0.607, 0.753, 0.688, 0.82} 
\readlist*\vm{0.7202727272727273, 0.22, 0.532, 0.747, 0.754, 0.81}

% lrv for learning rate value. Putting in command so that it doesn't ruin table alignment
\newcommand{\lrv}{\num{3e-5}}

\begin{table}
    \setlength{\tabcolsep}{0.4em}
    \setlength{\extrarowheight}{7pt}
    \sisetup{round-mode=places}
    \begin{center}
    \begin{tabular}{| c c | c c c c c c | S[round-precision=2] S[round-precision=2] S[round-precision=2] S[round-precision=2] S[round-precision=2] S[round-precision=2] |}
        \hline 
        \multicolumn{8}{|c|}{Model Configuration} & \multicolumn{6}{c|}{Results in MSE}\\
        \hline
        \nep & \mn & \nl & \dhid & \drop & \lr & \clip & \nh & Tot & \temp{} & \vel{} & {\dev{}} & \art{} & \ped{} \\ 
        % \nep & \mn & \nl & \dhid & \drop & \lr & \clip & \nh & 1 & 1 & 1 & 1 & 1 & 1 \\ 
        \hline 
123 & LSTM   & 3  & 256  & 0.1 & 0.1     & 0.5 &    & \a[1] & \a[2] & \a[3] & \a[4] & \a[5] & \a[6] \\ 
\hline
147 & T-BL   & 6  & 256  & 0.1 & 3e-5 & 0.5 & 6  & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
169 &        &    & 128  &     &         &     &    & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
128 &        &    & 528  &     &         &     &    & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
133 &        &    & 1024 &     &         &     &    & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
118 &        & 12 &      &     &         &     &    & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
--- &        & 24 &      &     &         &     &    & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
132 &        &    &      &     &         &     & 13 & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
171 &        &    &      & 0.2 &         &     &    & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
173 &        &    &      &     & 0.01    &     &    & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
--- &        &    &      &     &         &     & 26 & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
134 &        & 12 & 528  &     &         &     &    & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
--- &        & 12 &      &     &         &     & 13 & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
135 &        & 12 & 528  &     &         &     & 13 & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
125 &        & 24 & 528  &     &         &     &    & \b[1] & \b[2] & \b[3] & \b[4] & \b[5] & \b[6] \\
\hline
& HAN-BL &  - &  -   &  -  &    -    &  -  &  - & \vbl[1] & \vbl[2] & \vbl[3] & \vbl[4] & \vbl[5] & \vbl[6] \\
& HAN-S  &  - &  -   &  -  &    -    &  -  &  - & \vs[1]  & \vs[2]  & \vs[3]  & \vs[4]  & \vs[5]  & \vs[6] \\
& HAN-M  &  - &  -   &  -  &    -    &  -  &  - & \vm[1]  & \vm[2]  & \vm[3]  & \vm[4]  & \vm[5]  & \vm[6] \\
        \hline
    \end{tabular}
    \caption{A comparison of 3 different families of EMP generation models: virtuosoNet models, Transformer models, and our LSTM baseline models. The left side of the table presents the configuration for each of the models, exluding the virtuosoNet models which are present in other works \cite{jeong2019graph,jeong2019virtuosonet}. \nep{} is the ID of the Neptune experiment, \nl{} is the number of layers, \dhid{} is the dimension of the hidden layers, \drop{} is the dropout, \lr{} is the learning rate, \clip{} is the gradient clip, and \nh{} is the number of attention heads. The right side of the table presents the MSE results for all models along the five different expressive dimensions mentioned in \ref{sec:qualitative-eval-problems}, as well as the total MSE which is an aggregation of all the individual expressive features. The entries for the HAN models come from virtuosoNet and are given in \cite{jeong2019virtuosonet} }
    \label{tab:quantitative}
    \end{center}
\end{table}


\section{Qualitative Evaluation: Personal Analysis}
In section \ref{sec:qualitative-eval-problems} we outline the evolution of our research method and identify major setbacks in the evaluation and comparison of our models. In our experience, our own qualitative evaluation through listening tests proved to be the most useful method for guiding our model development and analysis. With the full acknowledgment of the inherent bias that underlines such a method and the need for better quantitative evaluation metrics, we will present some of our observations as the model development progressed. 

The first general observation is that the tempo of the Transformer model performances seemed to be much more dynamic than LSTM models, both for our own LSTM baseline as well as the virtuosoNet models. 





