\chapter{Methods and Experiments Results}\label{ch:ch5}
Given the proper background research and knowledge base, we will now describe our experiments and methods. Given the powerful advances in NLP and music generation tasks, our general goal was to investigate the Transformer model in application to EMP generation. Both language and music are by nature sequential and hierarchical. As such, one would expect the strengths of a Transformer model, created specifically for an NLP task, to have natural extensions into musical tasks. We use the complete end-to-end generation system of the \vnetf{}. This project's initial purpose was to determine if a Transformer-based model improves upon \vnet{}, given the same data, features, and evaluation metrics. However, as discussed in section \ref{ch:ch6}, answering that question proved to be more complicated than initially anticipated and led our research in a different direction. We present our initial method and experiment results in this chapter. 

There are three major reasons for adopting the \vnet{} system. 
\begin{enumerate}
    \item The KAIST dataset is the largest publicly available dataset used in EMP generation.
    \item \vnet{} models are the ``best performing'' in EMP.
    \item The data processing and model architecture code of the \vnetf{} are open-sourced\footnote{https://github.com/jdasam/virtuosoNet}.
\end{enumerate}
If we consider \vnet{} to be state-of-the-art in EMP generation, it provides a natural starting place to compare against any further model development. 

\section{Data and Features}
The \vnet{} system uses handcrafted features for both scores and performances, which are outlined in section \ref{ch:ch4}. We use the same feature set.

\section{Model}
In the \vnet{} system, there is a 1:1 mapping between notes in scores and performances. As mentioned in chapter \ref{ch:ch3}, encoder-decoder \seq{} models with variable-length input/output sequences account for the difference in length by using a generative decoder model which relies on the input encoding. Because our input and output sequences are the same lengths, we saw no need to include a generative decoder. Our model is a simpler encoder-only Transformer model, conceptually similar to BERT. Sitting on top of the Transformer encoder is a simple FFNN that uses the learned representation from the Transformer and maps to the final output feature set. We use the absolute positional embedding of \citet{vaswani2017attention} used with the original Transformer. The model architecture is shown in figure \ref{fig:model_architecture}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/ch5/model_architecture.jpg}
    \caption{Our model uses the same encoder Transformer architecture as shown by \citet{vaswani2017attention}. There are $N$ Transformer layers which are composed of multi-head attention and feed-forward mechanisms. There is a simple linear layer at the top of the architecture which creates the final mapping to the output features.}
    \label{fig:model_architecture}
\end{figure}

\section{Experiments and Model Evaluation}\label{sec:experiments-and-evaluation}
Our initial experiments were designed to answer two questions: 
\begin{enumerate}
    \item Does a Transformer based EMP generation model outperform a similar LSTM based EMP generation model?
    \item Does a Transformer based EMP generation model outperform existing state-of-the-art (\vnet{}) models?
\end{enumerate}
To answer both of these questions, we ran all of our experiments using the existing \vnetf{}. The \vnetf{} defines real-valued features, and the resulting models are regression models. All \vnet{} models use MSE as both the loss function for training and reconstruction error for quantitative evaluation. \vnet{} models are trained using an 8-1-1 train/valid/test data split, and \citet{jeong2019virtuosonet} present MSE results for each different parameter of the performance features on the test set. We follow the same method for our quantitative evaluation, using the same data split, loss function, and evaluation metrics. Our models were trained at 50 epochs, and the best model parameters were selected according to the lowest validation evaluation score. Our models were implemented in PyTorch using their native Transformer library. We used Neptune AI \cite{neptune} to manage our experiments and report the metric feedback. Data for all of our experiments, including the model hyperparameters and metrics, can be found online\footnote{\url{http://ui.neptune.ai/richt3211/thesis}}. 

Our main experimental goal was to find the best model configuration for a Transformer based model. To do so, we ran a reasonably exhaustive hyperparameter search for our Transformer model. To guide our search, we trained our baseline LSTM model and a few \vnet{} models in tandem with our Transformer model. We used the evaluation results from the validation set to gain an intuition about which family of models performed the best. The final test set evaluation happened after all model development was finished. 

\subsection{Quantitative Evaluation}
We use several different model configurations for the Transformer. Our Transformer baseline has 6 layers, 6 attention heads, and a hidden size of 256. We chose this as a base configuration because it closely matches the size of the original Transformer \cite{vaswani2017attention}, except for the hidden size of the feed-forward layer. We use a smaller hidden size of 256 for our base layer hidden size due to the relatively small dimensionality, 78, of the model's input data% 
\footnote{The original Transformer uses a baseline input dimensionality of 528. In this case, the model inputs are word vectors whose size can be configured dynamically to find the best word representation for a given model. In our case, our input size dimensionality is fixed according to the input features of the \vnetf{}}. We trained from scratch the Iterative Sequential Graph Network (ISGN) and the HAN baseline (HAN-BL) models as reported in \cite{jeong2019graph} and \cite{jeong2019virtuosonet} respectively and used the metrics from the validation data set to guide our own model development before we ran the final evaluation. %Table \ref{tab:quantitative} contains the result of the final evaluation.



The first major implication of the results is that the Transformer model easily outperforms the baseline LSTM but does not outperform the \vnet{} models. Within the Transformer models, the best performing model uses all of the same baseline Transformer parameters, except for a major increase in the hidden layer's dimensionality (model \tm{133}). Other models that perform well include those which only increase the number of attention heads (\tm{132} and \tm{26}) and those with an increase across several different parameters (\tm{135}). It appears that increasing the number of layers degrades performance (\tm{118}, \tm{181}, \tm{125}), and so does increasing the dropout and learning rates (\tm{171}, \tm{173}). 

Although we see improvements by increasing the hidden dimension and number of attention heads, the performance improvements aren't significant. There is only a difference of 0.03 in MSE between the best performing Transformer model and the Transformer baseline. In contrast, there is a difference of 0.11 between the best Transformer model and the best HAN model. An initial interpretation of these results would indicate that a Transformer model can't even match the existing state of the art in EMP generation (let alone beat it). As we discuss in later sections, this interpretation of results is not necessarily valid. 