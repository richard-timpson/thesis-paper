\chapter{Discussion} \label{ch:ch7}
The main research question we set out to answer was whether or not a Transformer model could outperform the existing state-of-the-art in EMP generation -- our experiment results do not indicate that is the case. We provide some hypotheses that explain the difference in model quality, which are centered around problems with the experiment method (evaluation) and our model architecture. The plausible explanations for a poorer performing model also form the foundation for future experiments in the area.

\section{Better Evaluation}
The most interesting observation from our experiment results is the mismatch between quantitative and qualitative evaluation. Although both evaluation methods identified the same model as the best performing, several mismatches were found in other experiments relating to our Transformer models. These mismatches may be explained by the particular \vnetf{} feature definitions which, as we showed in chapter \ref{ch:ch6}, directly affect the output of the MSE metric. Because the \vnetf{} MSE weighs pedal as more important than all other expressive parameters, a model which completely fails to output a decent tempo but excels at pedaling may be evaluated as a ``high-quality'' model. Such an evaluation is not desirable, as the tempo can negatively affect the listening experience. Any attempt to modify the metric calculation (as we did) or change the feature definitions makes the comparison interpretability of different models impossible. 

For these reasons, we call to question the validity of using MSE or other similar metrics for quantitatively evaluating EMP models. Although the current evaluation metrics may facilitate the proper evaluation for some aspects of performance, they are too dependent on the underlying feature definitions. The definition of features is an ongoing and evolving process that is an essential component of model development. The lack of consistent and standard evaluation methods in EMP performance (and musical performance in general) creates a problematic space for conducting useful experiments. Although this limitation can be overcome using a sophisticated qualitative evaluation involving listening tests by professional musicians, such an evaluation method is time-consuming, expensive, and potentially inaccessible. 

We again draw an analogy to current research methods in NLP and machine translation to provide some concrete suggestions for developing better quantitative evaluation metrics. MT used to face a similar set of evaluation problems that currently plague EMP. MT evaluation was mostly conducted by humans, which (although extensive) was too expensive, took too long, and wasn't consistent or reliable. The Bilingual Evaluation Understudy (BLEU) metric~\cite{papineni2002bleu} addresses these complications. The BLEU metric's two fundamental principles are that a computed translation's quality is determined by how closely it can match an existing human translation and that a single output translation should compare against multiple reference translations \emph{at the same time}. The BLEU metric is also computed by comparing the output text from MT models instead of directly using the output features that are often specific to a model, as does MSE. 

It would make sense for an EMP generation evaluation metric to loosely match the principles of BLEU. The first improvement to be made is to compare the final output representation (MIDI) rather than directly use the defined feature set. Using MIDI would allow easy comparison of models that use different feature representations and significantly improve the robustness (or lack) of experiment results. The other potential improvement would include comparing a single generated performance to several reference performances at the same time and reporting a single metric number that represents the aggregate reference comparison. Although the underlying mathematics of evaluation widely differs between EMP and MT, the general principles that guide the evaluation can be the same. 

\section{Possible Model Improvements}
With the understanding that our quantitative evaluation metrics may not properly indicate the quality of our models, we provide some hypotheses about why the Transformer model was outperformed by \vnet{}. They are centered around high-level components of the model architecture as opposed to the underlying mechanisms (attention in the Transformer and the hidden state in an RNN) for the different modeling domains. 

The first plausible explanation for the difference in performance between \vnet{} and our Transformer model is that \vnet{} models explicitly define the hierarchical layers of music into the model. The Transformer models, which are a straight implementation of the original architecture, have no such ``musical knowledge'' baked in. \vnet{} models are mostly composed of hierarchically based attention layers defined at note, beat, and measure levels\footnote{This type of hierarchical structure is something that we would expect the Transformer model to learn given its composition of stacked attention based layers. Although we never ran experiments to verify this, each layer should be learning a more abstract representation of the score.}. 

Our initial expectation was that we would see similar results because the Transformer outperforms other comparative models in NLP. We did see a significant improvement over our baseline LSTM model, which is architecturally similar to our Transformer model. This indicates that the attention mechanism in a Transformer does improve upon the hidden state recurrence mechanism of an RNN. However, it appears that the ``hardcoded'' musical information in \vnet{} models is more important than merely throwing a different type of sequential computation at the problem.

One interesting observation from the qualitative evaluation is that the Transformer models were more dynamic, both good and bad for the resulting performances. A model that is too dynamic (especially in tempo) lacks the cohesiveness that makes performance identifiable. Performance that isn't dynamic enough is musically uninteresting and the reason for using ML to build computational models in the first place, given the highly undesirable ``deadpan'' performances. Transformer models' dynamics indicate that they are potentially more expressive and creative, which are desired traits in generating novel performance. It could be the case that a Transformer architecture that explicitly defines hierarchical musical layers in the model results in performances that have both consistency and creativity. 

The other primary plausible explanation for the difference in model performance is that we lost a fundamental component of expressiveness by removing the Transformer decoder. We chose an encoder-only Transformer model based on the \vnetf{} definition of EMP as a one-to-one mapping between a note in the score and its corresponding expression in performance. We also chose this model because of the success of similar Transformer adaptations such as BERT~\cite{devlin2018bert}. As discussed in chapter \ref{ch:ch3}, the original \ed{} Transformer architecture maps between \emph{variable} length sequences. The decoder uses an internal representation of the input from the encoder as well as an autoregressive model to \emph{generate} from scratch the target sequence. Although our model learns a one-to-one mapping, we believe that the full \ed{} architecture may better model EMP in general. From a philosophical perspective, the \ed{} assumes that \emph{generation by the performer} is as fundamental a component of the performance process as is understanding a score. An encoder-only model assumes that performance is encapsulated only by apprehending the score. Removing the decoder cuts out creative freedom on the part of the performer.

\vnet{} is an encoder-decoder architecture and does not lose this fundamental component of performance, as does our model. We suspect that a full encoder-decoder Transformer architecture would have better capability to create both consistent and creative performance. Although it is possible to develop a model which explicitly defines the hierarchical layers as does \vnet{}, merely adding the decoder may be enough to build a better performing model% 
\footnote{We attempted to implement such a model after our initial experiments but ran into practical problems with the model dimensionality using PyTorch's native Transformer implementation}. 
