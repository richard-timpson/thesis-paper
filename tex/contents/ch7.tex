\chapter{Discussion} \label{ch:ch7}
\subsection{Evaluation}
The most interesting observation from our experiment results is the mismatch between quantitative and qualitative evaluation. Although both evaluation methods identified the same model as the best performing, several mismatches were found in other experiments relating to our Transformer models. These mismatches may be explained by the particular \vnetf{} feature definitions which, as we showed in chapter \ref{ch:ch6}, directly affect the output of the MSE metric. Because the \vnetf{} MSE weighs pedal as more important than all other expressive parameters, a model which completely fails to output a decent tempo but excels at pedaling may be evaluated as a ``high-quality'' model. Such an evaluation is not desirable, as the tempo can negatively affect the listening experience. Any attempt to modify the metric calculation (as we did) or change the feature definitions makes the comparison interpretability of different models impossible. 

For these reasons, we call to question the validity of using MSE or other similar metrics for quantitatively evaluating EMP models. Although the current evaluation metrics may facilitate the proper evaluation for some aspects of performance, they are too dependent on the underlying feature definitions. The definition of features is an ongoing and evolving process that is an essential component of model development. The lack of consistent and standard evaluation methods in EMP performance (and musical performance in general) creates a problematic space for conducting useful experiments. Although this limitation can be overcome using a sophisticated qualitative evaluation involving listening tests by professional musicians, such an evaluation method is time consuming, expensive, an potentially inaccessible. We present some possible solutions to the problem of evaluation in chapter \ref{ch:ch8}

\subsection{Possible Model Improvements}
With the understanding that our quantitative evaluation metrics may not properly indicate the quality of our models, we provide some hypotheses about why the Transformer model was outperformed by \vnet{}. They are centered around high level components of the model architecture as opposed to the underlying mechanisms (attention in the Transformer and the hidden state in an RNN) for the different modeling domains. 

The first plausible explanation for the difference in performance between \vnet{} and our Transformer model is that \vnet{} models explicility define the hierarchical layers of music into the model. The Transformer models, which are a straight implementation of the original architecture, have no such ``musical knowledge'' baked in. \vnet{} models are mostly composed of hierarchically based attention layers which are defined at note, beat, and measure levels\footnote{This type of hierarchical structure is something that we would expect the Transformer model to learn given its composition of severstacked attention based layers. Although we never ran experiments to verify this, each layer should be learning a more abstract representation of the score.}. 

Our initial expectation was that we would see similar results because the Transformer outperforms other comparative models in NLP. We did see an a major improvment over our baseline LSTM model which is architecturally similar to our Transformer model. This is an indication that the attention mechanism in a Transformer does improve upon the hidden state recurrence mechanism of an RNN. However, it appears that the "hardcoded" musical information in \vnet{} models is more important than merely throwing a different type of sequential computation at the problem.

One interesting observation from the qualitative evaluation is that the Transformer models were more dynamic, both good and bad for the resulting performances. A model that is too dynamic (especially in tempo) lacks the cohesiveness that makes performance identifiable. Performance that isn't dynamic enough is musically uninteresting and the reason for using ML to build computational models in the first place, given the highly undesirable "deadpan" performances. Transformer models' dynamics indicate that they are potentially more expressive and creative, which are desired traits in generating novel performance. It could be the case that a Transformer architecture that explicitly defines hierarchical musical layers in the model results in performances that have both consistency and creativity. 

The other major plausible explanation for the difference in model performance is that we u

Our definition of the weighted MSE may or may not be useful in moving towards better quantitative evaluation methods. It arose from our explorations into the possible effects that different model training components had on resulting performances. More than anything else, our weighted MSE experiments show that using MSE to evaluate performance is too dependent on the specific data and feature representation of a given model. Changing the data representation that the model uses should not have any effect on the resulting evaluation. When it does, as is the case with \vnet{}, building future models becomes difficult as there is no easy way to compare with the existing state-of-the-art. Qualitative evaluation is one potential method to overcome this limitation, but having such a dependency makes research potentially inaccessible.  

