\chapter{Discussion} \label{ch:ch7}
The most interesting observation from our experiment results is the mismatch between the quantitative and qualitative evaluation. The highly subjective nature of musical performance makes defining ``quality'' in computational performance difficult. However, according to quantitative and qualitative evaluations, \vnet{} models still perform better than all others presented in this work. Although there is a disparity between the two types of evaluation, it appears to be the case that there is some correlation between them. 

The difference in performance between the \vnet{} and Transformer models is probably best explained by the fact that \vnet{} explicitly defines the hierarchical layers of music into the models. The Transformer models, which are a straight implementation of the original architecture, have no such ``musical knowledge''\footnote{This type of hierarchical structure is something that we would expect the Transformer model to learn given its composition of several stacked attention based layers. Although we never ran experiments to verify this, each layer should be learning a more abstract representation of the score. }. \vnet{} models are mostly composed of hierarchically based attention layers which is conceptually similar to a Transformer model. Our initial expectation was that because the Transformer outperforms other comparative models in NLP, we would see similar results. It appears that the "hardcoded" musical information in \vnet{} models is more important than merely throwing a different type of sequential computation at the problem. 

One interesting observation from the qualitative evaluation is that the Transformer models were more dynamic, both good and bad for the resulting performances. A model that is too dynamic (especially in tempo) lacks the cohesiveness that makes performance identifiable. Performance that isn't dynamic enough is musically uninteresting and the reason for using ML to build computational models in the first place, given the highly undesirable "deadpan" performances. Transformer models' dynamics indicate that they are potentially more expressive and creative, which are both desired traits in generating novel performance. It could be the case that a Transformer architecture that explicitly defines hierarchical musical layers in the model results in performances that have both consistency and creativity. 

Our definition of the weighted MSE may or may not be useful in moving towards better quantitative evaluation methods. It arose from our explorations into the possible effects that different model training components had on resulting performances. More than anything else, our weighted MSE experiments show that using MSE to evaluate performance is too dependent on the specific data and feature representation of a given model. Changing the data representation that the model uses should not have any effect on the resulting evaluation. When it does, as is the case with \vnet{}, building future models becomes difficult as there is no easy way to compare with the existing state-of-the-art. Qualitative evaluation is one potential method to overcome this limitation, but having such a dependency makes research potentially inaccessible.  

