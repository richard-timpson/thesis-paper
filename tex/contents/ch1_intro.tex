\chapter{Introduction} \label{ch:introduction}

In 1952 L.A. Hiller and L.M Issacson ushered forth a new era of the study of both music and computer science when they introduced the Illiac Suite – the first composition that was created solely by a computer \cite{sandred2009revisiting}. What we'll refer to broadly as Music Information Retrieval (MIR)\footnote{\citet{widmer2016getting} points out that MIR itself does not encompass the entire scope of computer music research, but that it is a good proxy to use when referring the field as a whole. We will operate under the same assumption} research has continued to see impressive advancements since the introduction of the Illiac Suite in several different domains, including musical composition\cite{briot2017deep}, instrument and sound synthesis\cite{engel2017neural}, and musical analysis\cite{widmer2016getting}. Much of the challenge in MIR research is to bridge the gap between the highly subjective nature of the musical experience paired with the strong hierarchical and mathematical patterns that are present from a quantitative perspective. The most well known application of MIR research is that of musical recommendation platforms such as Spotify which study the different patterns present in a diverse range of musical ideas to suggest to their listeners music which music they may appreciate in the future given their past listening habits. 

Widmer\cite{widmer2016getting} suggests that there are a deeper set of problems that the MIR community should focus on which are by their nature, more fundamental to understanding the nature of music itself. One such problem is that of understanding what constitutes \emph{expression} in musical performance. Current automatic performance generation systems (typically bundled with musical notation software) render deterministic and uninteresting performances which don't contain the "human" element; that is, they do not use the different components of musical performance such as variations in timing, dynamics and articulation, to "express" different ideas or emotions. Each performance of a musical composition inherently carries with it some interpretation of the composition which is communicated through musical expression. Our work is a further continuation of the computational modeling of expressive musical performance (EMP) in the context of automatic performance generation. 

Typical EMP generation systems use Machine Learning (ML) to build generation systems which are trained on existing data corpuses comprised of actual human performance. The most recent models are usually either probablistic (usually using Hidden Markov Models) in nature, or based on artificial neural networks (ANN) which is a family of ML models that have led to the rapid increase of Artificial Intelligence (AI) systems in many areas, including computer vision, natural language processing (NLP), and speech and audio processing\cite{goodfellow2016deep}\footnote{The use of neural networks in ML is commonly referred to as "Deep Learning" because of the many connected layers that usually comprise the networks. The term "deep" is used to describe the long path which information must follow to make it's way through the large network. This is in contrast to other ML models which usually do not have that depth}. State of the art models are based on Recurrent Neural Networks (RNN) and their adaptations which are designed to model sequential data, such as music. A relatively new model in sequential Deep Learning (DL), the Transformer, has led to impressive advances over RNN based models in NLP and other fields. We use the Transformer in EMP generation, which to our knowledge has never been done, using an existing end to end state of the art EMP generation system. 

is a subset of general CM research and CM research presents a unique challenge to both musicology and computer science due to the highly subjective nature of music paired with the strong quantitative and mathematical nature of computer science. However, music is also inherently mathematical and contains a strong hierarchical structure from which powerful patterns emerge - how it is that these common patterns lead to such a highly subjective human experience is outside of the scope of this work. It is perhaps due to the inherently paradoxical nature of music that it creates such an interesting set of problems to study, particularly from a computational perspective. In the authors' opinion, this problem set is one of the most worthwhile to study in the current day, and should receive more focus in the research literature. 

To reach such a point, it is necessary to view the field from the lens of Artificially Intelligent musical systems that are able to reason themselves about music. In general, Artificially Intelligent systems have seen immense progress in the last decade due to the rise of Machine Learning (particularly with Deep Learning) and it's applications in several different domains. Music has been one of these domains and has seen impressive advances in several musical tasks such as musical composition\cite{huang2018music}, and musical analysis\cite{widmer2016getting}. 

One of the more intriguing problems in computer music is the creation of an expressive performance generation system. There are several commercially available notation and playback software systems \footnote{\url{musescore.com} and \url{www.finalemusic.com}} that are able to automatically generate musical performances from a purely symbolic musical representation in the form of a score (more commonly known as sheet music). The systems are built based on a predefined set of rules that create deterministic performances given a score. Although the performances are technically an "accurate" rendering of the score, they don't contain the \emph{human} element. That is, they contain a straight and deterministic mapping between a note marked in a score and its corresponding position in the associated performance. In such systems, there is no \emph{expression} of the performance. As such, these systems produce robotic-sounding performances that are offputting to human listeners \footnote{an example performance can be heard from \href{https://musescore.com/user/33884420/scores/6466906}{musescore}}. 

These simple performance generation systems do not render "expressive" because they don't account for performance features that add the human element. Such features include variations in tempo, timing, and dynamics (TODO: Add a reference to a later section that will go over each feature in detail). A performer of a composition uses each of these features of the performance to add a unique interpretation of the piece. This interpretation (or expression) is responsible for providing the "human" element of musical performance. 

This poses the question of whether or not a computer system can create expressive performances that are close to actual human performance (or even creating a completely new style of expression that a human cannot). This has been an active area of computer music research since the 1980s \cite{friberg2006overview}. This thesis is a further exploration of the current research with a particular emphasis on applying the current state of the art Deep Neural Network architectures from Natural Language Processing (NLP) research to the problem domain and understanding their effects. \rtodo{Revisit introduction after outline and more writing}

\begin{itemize}
    \item Introduce the idea of musical research with computers. Talk about the illiac suite \cite{sandred2009revisiting} and Music Information Retrieval.
    \item Significance of machine learning on the field 
    \item Introduce idea of expressive musical performance. Brief conversation about the different performance components (articulation, dynamics, timing). 
    \item Using Transformer architecture which hasn't been done in the field. 
    \item \rtodo[,inline]{report results}
\end{itemize}