\chapter{Introduction} \label{ch:introduction}

In 1952 L.A. Hiller and L.M Issacson ushered forth a new era of the study of both music and computer science when they introduced the Illiac Suite – the first composition that was created solely by a computer \cite{sandred2009revisiting}. What we'll refer to broadly as Music Information Retrieval (MIR)\footnote{\citet{widmer2016getting} points out that MIR itself does not encompass the entire scope of computer music research, but that it is a good proxy to use when referring the field as a whole. We will operate under the same assumption} research has continued to see impressive advancements since the introduction of the Illiac Suite in several different domains, including musical composition\cite{briot2017deep}, instrument and sound synthesis\cite{engel2017neural}, and musical analysis\cite{widmer2016getting}. Much of the challenge in MIR research is to bridge the gap between the highly subjective nature of the musical experience paired with the strong hierarchical and mathematical patterns that are present from a quantitative perspective. The most well-known application of MIR research is that of musical recommendation platforms such as Spotify which study the different patterns present in a diverse range of musical ideas to suggest to their listeners music which they may appreciate in the future given their past listening habits. 

Widmer\cite{widmer2016getting} suggests that there are a deeper set of problems that the MIR community should focus on which are by their nature, more fundamental to understanding the nature of music itself. One such problem is that of understanding what constitutes \emph{expression} in musical performance. Current automatic performance generation systems (typically bundled with musical notation software) render deterministic and uninteresting performances which don't contain the "human" element; that is, they do not use the different components of musical performance such as variations in timing, dynamics and articulation, to "express" different ideas or emotions. Each performance of a musical composition inherently carries with it some interpretation of the composition which is communicated through musical expression. Our work is a further continuation of the computational modeling of expressive musical performance (EMP) in the context of automatic performance generation. 

Typical EMP generation systems use Machine Learning (ML) to build generation systems which are trained on existing data sets comprised of actual human performance. The most recent models are  either probabilistic (usually using Hidden Markov Models) in nature, or based on artificial neural networks (ANN) which is a family of ML models that have led to the rapid increase of Artificial Intelligence (AI) systems in many areas, including computer vision, natural language processing (NLP), and speech and audio processing\cite{goodfellow2016deep}\footnote{The use of neural networks in ML is commonly referred to as "Deep Learning" because of the many connected layers that usually comprise the networks. The term "deep" is used to describe the long path which information must follow to propogate through the large network. This is in contrast to other ML models which usually do not have that depth}. State of the art models are based on Recurrent Neural Networks (RNN) and their common adaptation as a Long Short Term Memory (LSTM) network which are designed to model sequential data, such as music. A relatively new model in sequential Deep Learning (DL), the Transformer, has led to impressive advances over RNN based models in NLP and other fields. We apply the Transformer in EMP generation, which to our knowledge has never been done, using an existing end to end state of the art EMP generation system. 

We evaluated our model quantitatively through the current standard evaluation metric and it performs worse than an existing RNN based system. However, a qualitative evaluation through our personal listening revealed a disconnect between the performance according to the quantitative metric and performance according to actually listening to the performances. We ran further experiments deviating from the standard method to identify why this might be the case, and provide some insights about possible problems with the quantitative evaluation and intuition for how to create better evaluation methods. We also give an error analysis of our own model and notice that the decrease in performance may not be with the underlying Transformer mechanisms but with our network architecture from a higher level. 

We also bring to light some of the philosophical conundrums with studying EMP generation at a computational level. Because the "quality" of a musical experience is so subjective, creating the right incentives for a computer model to generate novel performances is not trivial. This is related to our discovery of possible flaws in our model evaluation. In agreement with other authors \cite{widmer2016getting}, we advocate for further research in the area to draw just as much from music at the human psychological level as the mathematical and statistical. 