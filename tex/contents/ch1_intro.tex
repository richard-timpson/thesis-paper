\chapter{Introduction} \label{ch:ch1}

In 1952 L.A. Hiller and L.M Issacson ushered forth a new era of the study of both music and computer science when they introduced the Illiac Suite – the first musical piece composed solely by a computer~\cite{sandred2009revisiting}. What we'll refer to broadly as Music Information Retrieval (MIR)~\footnote{~\citet{widmer2016getting} points out that MIR itself does not encompass the entire scope of computer music research but that it is a good proxy to use when referring to the field as a whole. We will operate under the same assumption} research has continued to see impressive advancements since the introduction of the Illiac Suite in several different domains, including musical composition~\cite{briot2017deep}, instrument and sound synthesis~\cite{engel2017neural}, and musical analysis~\cite{widmer2016getting}. Musical recommendation systems are the most commonly known application of MIR. They extract common musical patterns from different genres and songs to suggest new music to listeners, which they may appreciate in the future given their past listening habits%
\footnote{\href{https://www.spotify.com/us/}{Spotify} is the most well-known commercial platform that implements musical recommendation systems.}. Much of the challenge in MIR research is to close the gap between music's phenomenological nature as experienced by human listeners and the hierarchical and mathematical quantitative musical patterns present in nature.

\citet{widmer2016getting} suggests that there are a set of deep problems that the MIR community should focus on, which by their nature, are more fundamental to understanding the nature of music itself. One such problem is the exploration of what constitutes \emph{expression} in musical performance. Current commercial automatic performance generation systems render deterministic and uninteresting performances that don't contain the ``human'' element. These ``deadpan'' performances do not use the different musical performance components such as variations in timing, dynamics, and articulation to ``express'' creative musical ideas or emotions%
\footnote{\href{https://musescore.org/en}{MuseScore}, a popular open-source musical notation software, is one such commercial system. MuseScore, and other notation software programs like it, generate \href{https://musescore.com/user/33884420/scores/6466906}{deadpan performances}.}. Every musical performance results from an interpretation of a composition, and this interpretation is communicated through expressive performance. Our work is a further continuation of the computational modeling of expressive musical performance (EMP) in the context of automatic performance generation. 

Typical EMP generation systems use Machine Learning (ML) to build computational models trained on existing data sets comprised of actual human performance. Recent ML models are either probabilistic (usually using Hidden Markov Models) in nature or based on artificial neural networks (ANN). Deep Learning (a common term used to describe ML involving ANNs)% 
\footnote{The use of neural networks in ML is commonly referred to as ``Deep Learning'' because of the many connected layers that comprise the networks. The term ``deep'' describes the long path which information must follow to propagate through the extensive network. Typical ML models usually do not have that depth} has led to the widespread success of Artificial Intelligence (AI) systems in a variety of applications, including computer vision, natural language processing (NLP), speech processing, and audio processing~\cite{goodfellow2016deep}. State-of-the-art EMP models are mostly comprised of Recurrent Neural Networks (RNN) and their common adaptation as a Long Short Term Memory Network (LSTM). RNNs are designed to model sequential data (see chapter \ref{ch:ch2}), such as music. A relatively new model in sequential Deep Learning, the Transformer, has led to impressive advances over RNN based models in NLP~\cite{devlin2018bert,brown2020language} and other sequential data modeling problems~\cite{dosovitskiy2020image}. We apply the Transformer to EMP generation, which to our knowledge is the first such application, using an existing end-to-end state-of-the-art EMP generation system. 

We evaluated our model quantitatively using standard evaluation metrics, and it performed worse than the existing RNN based state-of-the-art. However, a personal listening test revealed a disconnect between quantitative and qualitative evaluation. We ran further experiments deviating from standard evaluation methods to identify why this might be the case. This experimentation led to insights about potential problems with current evaluation metrics and intuition for creating better evaluation methods. Our intuition comes from an error analysis of our model, which revealed that the performance decrease might not be attributed to the underlying Transformer mechanism but our high-level network architecture. 

We also bring to light some of the philosophical conundrums present when considering EMP generation from a computational level. Because the ``quality'' of a musical experience is highly subjective, creating the right incentives for a computer model to generate novel performances is not trivial. In agreement with other authors~\cite{widmer2016getting}, we advocate for further research to draw just as much from music at the human psychological level as the mathematical and statistical. 