\chapter{Introduction} \label{ch:ch1}

In 1952 L.A. Hiller and L.M Issacson ushered forth a new era of the study of both music and computer science when they introduced the Illiac Suite – the first musical piece composed solely by a computer~\cite{sandred2009revisiting}. What we'll refer to broadly as Music Information Retrieval (MIR)~\footnote{\citet{widmer2016getting} points out that MIR itself does not encompass the entire scope of computer music research but that it is a good proxy to use when referring to the field as a whole. We will operate under the same assumption} research has continued to see impressive advancements since the introduction of the Illiac Suite in several different domains, including musical composition\cite{briot2017deep}, instrument and sound synthesis\cite{engel2017neural}, and musical analysis\cite{widmer2016getting}. Musical recommendation systems are the most commonly known application of MIR, which use patterns across different genres and song to suggest new music to listeners which they may appreciate in the future given their past listening habits\footnote{\href{https://www.spotify.com/us/}{Spotify}     is a platform that implements musical recommendation systems.}. MIR research is challenged by a gap between music's phenomenological nature as experienced by human listeners and the hierarchical and mathematical quantitative musical patterns present in nature.

\citet{widmer2016getting} suggests that there are a set of deep problems that the MIR community should focus on, which by their nature, are more fundamental to understanding the nature of music itself. One such problem is that of understanding what constitutes \emph{expression} in musical performance. Current commercial automatic performance generation systems render deterministic and uninteresting performances that don't contain the "human" element -- they do not use the different musical performance components such as variations in timing, dynamics, and articulation to "express" creative musical ideas or emotions. Every musical performance is produced by some interpretation of a composition, and this interpretation is communicated through expressive performance. Our work is a further continuation of the computational modeling of expressive musical performance (EMP) in the context of automatic performance generation. 

Typical EMP generation systems use Machine Learning (ML) to build computational models trained on existing data sets comprised of actual human performance that are capable of rendering novel performances. Recent ML models are either probabilistic (usually using Hidden Markov Models) in nature or based on artificial neural networks (ANN). Deep Learning (a common term used to describe ML involving ANNs)\footnote{The use of neural networks in ML is commonly referred to as "Deep Learning" because of the many connected layers that usually comprise the networks. The term "deep" is used to describe the long path which information must follow to propagate through the large network. This is in contrast to other ML models which usually do not have that depth} has led to the rapid success of Artificial Intelligence (AI) systems in a variety of applications, including computer vision, natural language processing (NLP), speech processing, and audio processing\cite{goodfellow2016deep}. State-of-the-art EMP models mostly use Recurrent Neural Networks (RNN) and their common adaptation as a Long Short Term Memory Network (LSTM), designed to model sequential data, such as music. A relatively new model in sequential Deep Learning, the Transformer, has led to impressive advances over RNN based models in NLP~\cite{devlin2018bert,brown2020language} and other sequential data modeling problems~\cite{dosovitskiy2020image}. We apply the Transformer to EMP generation, which to our knowledge has never been done, using an existing end-to-end state-of-the-art EMP generation system. 

We evaluated our model quantitatively using standard evaluation metrics, and it performed worse than the existing RNN based state-of-the-art. However, a qualitative evaluation through personal listening revealed a disconnect between the performance according to the quantitative metric and performance according to human feedback. We ran further experiments deviating from standard evaluation methods to identify why this might be the case. We provide insights about potential problems with current quantitative evaluation and intuition for creating better evaluation methods. This intuition comes from an error analysis of our model, which reveals that the decrease in performance may not be with the underlying Transformer mechanisms but with our network architecture from a higher level. 

We also bring to light some of the philosophical conundrums present when considering EMP generation from a computational level. Because the "quality" of a musical experience is highly subjective, creating the right incentives for a computer model to generate novel performances is not trivial. In agreement with other authors \cite{widmer2016getting}, we advocate for further research to draw just as much from music at the human psychological level as the mathematical and statistical. 