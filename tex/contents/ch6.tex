\chapter{Discussion} \label{ch:ch6}
As we have outlined, this project underwent an unexpected evolution of purpose. Our initial goal was to determine if we could outperform the existing state of the art system in EMP generation with the exact same research methods and only a change of computational model. The initial quantitative experiment results indicated that a Transformer does not outperform hierarchical based recurrent models. However, when we ran our own subjective evaluation of the comparison in performance among the two families of models, we determined that the definitive declaration of one model being better than another was not so straightforward. This led us to question the validity of the original research method, particularly with the evaluation metric, and shift our own toward more nuanced and discovery-based experiments. The experiments themselves were not guided by any strict method, and as such, we cannot provide results that we consider to be robust or reliable. However, we would like to use our experience in running these experiments to provide some suggestions for the direction of future work in the area. 

\section{Directions for Future Work}
Our two general suggestions for research are centered on the main contributions of this work; that is, modeling and evalution. Although our Transformer based model was "outperformed" by the existing recurrent model, we feel that the Transformer model family still has the potential to improve upon recurrent models given the right architecture. As far as evaluation is concerned, we use our experience in our model development to consider the possbilities for what a better evaluation metric might look like. 

\subsection{Modeling: Performance as a Generative Process}
In section \ref{sec:experiments-and-evaluation} we present our proposed Transformer model and the reasoning behind the model selection. This reasoning was based upon the fact that the job of our model was to learn the one-to-one mapping between a single note in a score and it's corresponding expression in a performance, as well the success of similar Transformer adaptations such as BERT\cite{devlin2018bert}. This is in contract to the original encoder-decoder Transformer architecture, whose purpose in machine translation is to learning the mapping between \emph{variable} length sequences. In the encoder-decoder architecture, the job of the encoder is find some good representation of the input data (the original language) that it presents the decoder, who uses this data representation along with it's own internal representation of the output data (the translation language) to \emph{generate} from scratch the target sequence. Although for our particular formulation of EMP generation (based on the existing feature design of virtuosoNet) there is a one-to-one mapping between a score note and a performance note, we believe that the encoder-decoder architecture would be more appropriate for performance generation than our proposed encoder only model. The former assumes that \emph{generation on the part of the performer} is as fundamental a component of the performance process as is understanding the score. The latter assumes that performance itself is only a matter of correctly understanding and rendering the information presented in the score, and doesn't learn the fundamental aspects of performance itself. Given that our model implements the latter, we believe that explains both the lack of expression and perceived error in creating novel performances, as well as the lower performance according to the quantitative metric. 

virtuosoNet itself is based on an encoder-decoder architecture, along with the option to encode a specific performance style (which we feel is useful, but not necessary for performance in general). It uses a combination of pre-defined musically informed hierarchical boundaries (starting from measure, to beat, and ending in a single note) along with the attention mechanism to build both an LSTM based score encoder and generative performance decoder. We believe that it is the encoder-decoder architecture that explains the different in performance between virtuosoNet and our proposed model, and not necessarily indicative that recurrence mechanisms (LSTM) outperform attention mechanisms (Transformer). We suspect that a full encoder-decoder Transformer architecture would have the capability using attention alone to learn the hierarchical boundaries of music that are handcrafted into virtuosoNet. Because the Transformer is more unsupervised from that perspective, it's possible that it can learn additional hierarchical levels that are important in performance. Such a model (whose results may or may not outperform a recurrence based model) would be more useful from an analytical perspective, not only a generative one.  

It would also be useful to experiment with the positional embeddings and relative attention mechanisms that are part of the Music Transformer \cite{huang2018music} which is used to generate both composition and performance together (see \rtodo{Add section about Music Transformer}). The increase in direct performance generation of the Music Transformer using relative position attention as opposed to absolute position based attention would likely apply to EMP generation based on a score.  

The implementation of a full Transformer and the experimentation with relative attention are the next immediate steps for future work. We did attempt to implement the full Transformer model but ran into practical training problems using the native implementation of the Transformer in PyTorch, which is heavily based upon classification of text data which uses highly dimensionional word embeddings. We plan to continue work in this area using custom adaptations and possible innovations of the original Transformer, using the knowledge gained from this work to guide our experiment development. 

\subsection{Evaluation: Towards Better Metrics}
As has been lengthily discussed, we believe current quantitative methods for EMP generation are in need of significant improvement. Current methods are based upon comparing a predicted performance against an actual performance and calculating some overall numerical distance between the output features, usually as the Mean-Squared-Error. As we have brought to light, this score is going to be highly dependent on the featurization of performance expression. In our case, the performance features carried more information about the various aspects of the sustain and soft pedals than all other expressive features. Using MSE with this feature set will bias the evaluation towards models with pedal over other import features relating to tempo, timing and articulation. A different set of output features fundamentally changes the interpretability of the model and makes the comparison of models with different output features impractical. 

A direct comparison of a predicted performance with a single target performance will also create a strong bias toward the human performers interpretation in that performance. An evaluatoin on a large scale such dataset such as ours that has multiple performances for a single score will naturally account for some of this bias by presenting multiple "correct" interpretations for a single score and rewarding those models which can create performances that have commonalities between them all. However, the extent to which this bias exists is difficult to account for in interpreting evaluation results. This is especially dependent on the performances that exist in the evaluation set. For example, two of the scores in our test data set are Bach's Prelude and Fugue in F Major (BWV 858) and Chopin's Etude Op 10. No 2. There are 2 performances of Bach's Prelude and Fugue and 11 performances of Chopin's Etude. Does this mean that evaluation will require more generality for the models performance of Chopin's Etude than it will for Bach's Prelude and Fugue? If generality across composers and performance styles is desired (which for us is the case), how much can trust the model evaluation given this knowledge of the test data? 

Qualitative evaluation of models is used to address the potential problems of using numerical methods to measure performance. Although the qualitative evaluation methods are also subject to their own heavy bias and potential lack of consistency across multiple experiments, they do provide the "human" element of evaluation which produces an additional level of confidence in the result of the models. Of course, qualitative evaluation methods present a slew of their own practical concerns in evalution. Not only does the diversity of musical experience and knowledge of the listeners create a large space for interpretability of the results\footnote{Some evaluations present performance to experienced and profesional musicians \cite{oore2020time}, while others use student musicians \cite{jeong2019virtuosonet}. A case can be made using lay people with no formal musical education as the listeners is also a valid method, considering the lack of musicologolical bias they would hold.}, it is also difficult to gather together a group (no matter their background) of people who are willing to particpate in the listening evaluation. In our case we didn't have the time or the resources to put together such an evaluation. This in combination with the lack of confidence we can place in the quantitative metric made conducting research difficult and frustrating. 

These issues point to the strong need for more standardization in the feature engineering, data sets, and evaluation methods of EMP generation models. We again draw from our comparison of EMP generation to machine translation to draw some insight on how to develop more standardized methods.
\rtodo[,inline]{Continue discussion of better evaluation methods. Need to do more research on current methods}

\section{Looking Forward: Finding the \emph{Essence} of Performance}
As we went about conceptualizing and developing this project, one thought that has plagued us is that we don't have a proper understanding of the fundamental components that comprise musical performance. Widmer in his \emph{Con Espressione}\footnote{\emph{Con Espression} is the Italian phrase for "with feeling" and is used as direction in musical notation} Manifesto\cite{widmer2016getting} emphasizes the importance of focusing on the finding the \emph{essence} of music itself and using that deeper understanding to more impactful technology related to MIR. When we first started the project we thought that it would be relatively simple to throw the powerful attention mechanism at a fundamentally sequential based modelling problem and that we would see improved results. It is only after running our experiments and taking a step back to look at the results that we understand what Widmer means when he refers to the \emph{essence} of music. 

Our experience in the development of this project further convinces of the fact that the computational study of music is an inherently difficult problem, not because of the limits of computation, but because of our current limited understanding of what music actually is. It is relatively simple to experience music on a personal level and to share in that experience with others. However, it is our (and Widmer's) conjecture that there is a fundamental disconnect between our understanding of the phenomenological aspect of music and the actual statistical patterns of nature that make it so appealing. MIR research attempts to encode these statistical patterns in computation and as such, deriving results that are meaningful in practical application with real human interaction is non-trivial. Widmer's suggestion is that we focus on gaining better understanding of the relationship between music in nature and music as it is perceived, and we echo that sentiment here. 

This may be an essential component of deriving better evaluation systems for performance, which can hopefully drive the future development of performance generation models. To create a proper evaluation system or metric, we first need to understand \emph{what exactly it is that constitutes a "good" or "bad" musical performance to a human listener}. This is a separate question from determining the quality of a musical composition, or even of the synthesis of a performance (whether it is the form of an acoustic instrument or a digitial synthesizer). To us, it is not clear where to draw the exact line between composition, performance, and synthesis from the perspective of the human listener. Although we conceptualize them as independent from each to mathematically define our problem space separate as shown in Figure \ref{fig:generation_process}, they may in fact be entirely dependent or even the same phenomena expressed differently through nature. For example, a musician performing a Jazz improvisation on a guitar may use the physical process of synthesizing sound, such as the way he strikes the guitar string or bends the string to reach a particular note, as driving factors in the musical piece. It is the actual physical limitation of the guitar instrument as the driver of creating sound that enables him to create musical subtleties in both the spontaneous composition and performance of improvisation. Is there a clear cut line between what constitues composition, performance, and synthesis in such a case?

On the flip side, we can analyze over 1000 symbolic musical compositions by Johann Sebastian Bach and their many musical adaptations over the last several hundred years. One example is the well known adaptation of his Prelude No. 1 in C Major, BWV 846 published in 1722 as the accompaniment to a melody composed by Charles Gounod and set to the lyrics of the well known Latin prayer, \emph{Ave Maria}. The original arragement, published in 1853 was for violin (or cello) with the piano\footnote{For an example performance, look \href{https://www.youtube.com/watch?v=hyUhEjtlDLA&ab_channel=YoYoMaVEVO}{here} for a recent performance by the well known cellist, Yo-Yo Ma}, but it has since been arranged and performed countless times for different instrument including guitar, string quartet, piano solo, solo vocal and full choir. It is clear that we can view the original piano Prelude as it's own composition and separate from the many different adaptations in composition, performance, and synthesis that complete the full musical experience. 

All of this is to say a good definition for what makes the \emph{essence} of musical performance may be impossible to define without further musical exploration. We would like to adopt Widmer's philosophy to guide our future explorations and draw from the fields of musicology, music pyschology, and music cognition as well as further advancement in computer science and mathematics to come closer to discovering this essence. From this work, we have learned that using the attention mechanism and Transformer models in EMP generation creates much more dynamic performances, both "good" and "bad", than recurrence models. The \emph{essence} of this finding could be that using the long term memory that attention provides allows more creative freedom in the performance process than the shorter term memory of an LSTM which might be more constrained by global score features such as the overall tempo. It also may be that having too much creative freedom without respect for global conditioning breaks the inherent musical boundaries defined by congitive perception, as we found to be the case with our Transformer models which were so fast that they were "unlistenable". Further experiments with modeling, data gathering, feature extraction, and evaluation, as well as an exploration of what determines a quality musical experience from a musicological and human cognitive perspective, will help answer these questions. It is our hope that we can continue to explore these problems (difficult as they may be) using additional perspectives drawing from musical research, to come closer to finding the \emph{essence} of performance, and music as a whole. 

% \begin{itemize}
%     \item give brief overview of experiment and results. General story is, quantitative values indicated model was worse, but we weren't confident in the metric. Tried to use different experiment methods to find better models and were able to come up with what we believe to be better models, but it is hard to know since our viewpoint is so subjective. We believe that the existing models are better than ours, but that does not mean that there are not holes in both systems. In some ways our models are better and in other ways existing models are better. The lack of a consistent and reliable metric to measure performance makes both the development and interpretation of results difficult. Would have been nice to do a real qualitative evalution but didn't have the resources to do so. That is another indication for the importance of better metrics. 
%     \item Possibilities for error and improvements in models. We didn't do a strong analysis of the data processing and feature engineering. It is likely that improvements along this front with the same existing models would yield better results. Would be helpful to have cross domain research with musicology to guide this work. Our model was an encoder only seq2seq model, which takes out the generative auto-regressive side of the Transformer decoder. We initially thought of performance as simply learning the mapping between a score and expressive features. It may be better to think of it more like machine translation, wherein there is some encoding or interpretation of a score, but also an inherent generative and creative process in creating a performance. The decoder side of the transformer has been used for pure expressive performance generation, and it's likely that using a full Transformer model would fit better with the natural state of performance generation. This is how the virtuosoNet architecture is structured, and that component of the architecture may alone account for it's improvement in performance. We tried to run experiments with a full Transformer architecture but ran into practical problems using PyTorchs native Transformer implementation, given that it was initially designed for use with language and word embeddings. It should be possible to use the full architecture and is the immediate next step in future work. 
%     \item Intuition towards better evaluation metric. As discussed, it may be useful to conceptualize performance as similar to translation, in which the entity uses the input (score or source sentence) to guide it's own actual generative process (performance or target sentence). As with performance, there is not one "correct" way to translate language and the quality of a translation is inherently subjective. One of, if not the biggest, breakthroughs in machine translation was the advent of the BLEU evaluation metric. The idea behind this metric was to simultaneously compare a translated sentence to several real human translations \emph{at the same time}, and use an aggregate of the similarities between the predicated translation and actual translation. What performance generation needs is a metric similar to BLEU. It may be useful to draw from the same ideas in BLEU, but it also may be likely necessary to use a heavy influence from musicology and music pyschology to let the human cognitive aspects of music perception to guide the evaluation of performance models. 
%     \item The difficulties in running the experiments are also another testament to the inherent challenge of the problem itself. The problem was much more complex than we initially imagined it to be, and it may be necessary to develop novel model architectures that match the nature of music itself. Convolutional NNs have done this with Computer Vision, Transformers have done this for NLP. What will the "Transformer" architecture for music look like? To drive the development of such a model, a deep understanding of both music and data science will be required. Further research into the field may benefit from a focus on gaining this deeper understanding than in applying existing methods from other domains. This is similar to the sentiment that Widmer shares. 
% \end{itemize}