\chapter{Analysis} \label{ch:ch6}
Our initial experiments using a quantitative evaluation indicated that our Transformer model does not improve the state of the art in EMP generation. As a method of "debugging" our Transformer model to look for potential errors and improvements, we wanted to actually listen the performances generated by our models to identify any problems, if they existed at all. 

These listening tests acted both as an error analysis as well as our qualitative evaluation of the models. It became apparent to us right away that there was a disconnect between the quality of the model according to the quantitative metric and our own evaluation while listening to performances. In this chapter, we present this analysis. It includes the method used for our qualitative evalution, new experimental methods to improve our own models, and conclusions about the quality of each model according to our listening tests. 


%tm for transformer model

\section{Identifying Training Problems}\label{sec:qualitative-eval-problems}
To conduct our listening tests, we first had to use the trained models to generate performance MIDI files. We generated performances for a variety of our trained models using six compositions from three different composers, listed in table \ref{tab:compositions}. The MIDI performance files are available for download using the Neptune AI web interface. For easy comparison of each model's performance, we loaded their relevant MIDI files into the Digital Audio Workstation (DAW) Logic Pro X. We used their "Steinway Grand" instrument synthesizer to create the final audio. Logic Pro X also provides visualizations of different performance aspects, including the MIDI velocity of each note and the sustain and soft pedals' value throughout the performance. \footnote{Performances generated by models can be view through our \href{https://ui.neptune.ai/richt3211/thesis/experiments}{Neptune Project}. Each experiment has an ID and we've run performance generation code for many of the models. To listen to performances, visit an experiments' artifacts tab and download available performance MIDI files. These MIDI files can be played in any DAW software, such as Logic Pro X. If performances don't exist for an experiment, contact the autor} \rtodo[,inline]{Add reference to image with logic pro x visualizations}. We used these visualizations in tandem with our interpretation of the generated audio to guide our analysis. 

\begin{table}
    \setlength{\extrarowheight}{3pt}
    \begin{center}
    \begin{tabular}[]{| l | l |}
        \hline
        Composer & Composition \\ 
        \hline 
        Bach & Prelude in E Minor, BWV 855 \\
        Bach & Prelude in F-sharp Major, BWV 858  \\ 
        Chopin & Etude Op. 10, No. 12 \\ 
        Chopin & Fantaisie-Imropmptu \\ 
        Beethoven & Piano Sonata No. 17 First Movement \\ 
        Mozart & Piano Sonata No. 11 First Movement \\ 
        \hline
    \end{tabular}
    \caption{The compositions used for the qualitative evaluation of our models. All scores come in the form of MusicXML from MuseScore. None of the scores were present in the training data}
    \label{tab:compositions}
    \end{center}
\end{table}

As mentioned earlier, it was apparent that there was a mismatch between a model's performance quality according to the MSE value and our evaluation. For example, the Transformer model with $N_{id}$ 125 (which we will denote as \tm{125}, see Table \ref{tab:quantitative}) had much worse validation MSE metrics than almost every other model Transformer model. However, our listening test revealed that the absolute tempo for smaller models, such as the Transformer baseline \tm{147}, was much faster and sounded worse (to the point where the performances are almost 'unlistenable') than \tm{125}. The potential disconnect between the 'quality' of the model as determined by quantitative and qualitative evaluation led us to investigate possible problems with the training methods used by \citet{jeong2019virtuosonet}. 

One of the first potential problems we identified was the method used to calculate and interpret the loss and evaluation. The output features of virtuosoNet are a sequence of vectors with a length of 11. The first four features correspond to a single component of expression. They are: tempo, velocity, deviation, and articulation, respectively. The last seven features are all different numbers that correspond to information about the pedal \cite{jeong2019score}. \citet{jeong2019virtuosonet} present MSE metrics for five different expressive parameters, which include all of those previously mentioned, as well as the pedal. Both \citet{jeong2019virtuosonet} and our references to pedal MSE are an aggregation of the seven different features that contain pedal information. In contrast, the other 4 MSE metrics only apply to a single feature. 

The original MSE used to train virtuosoNet assumed that every feature of the output vector contributed equally to the final output and corresponding loss optimization. Given that there is seven times more information for the pedal parameter than all others, we can conceptualize what we will call \vnetf{} loss as placing more importance on the pedal than every other expressive feature. Given that the \vnetf{} MSE loss is used both for model training and evaluation, this means that it inherently rewards those models which express pedal better than all other performance features. If the \vnet{} models outperformed our Transformer models according to this metric, does that mean that they are better at modeling EMP generation as a whole, or that they are simply more fit to model the pedal? To answer this question, we came up with a new weighted MSE loss function that allows for a more fair representation of the evaluation method. 

\newcommand{\rvec}[1]{\mb{#1}}

% vn for vector normal. predicted
% vh for vector hat. target
\newcommand{\vn}{\rvec{v}}
\newcommand{\vh}{\rvec{\hat{v}}}

% df for difference
\newcommand{\df}[1]{(\vn_{#1} - \vh_{#1})^2}

% al for alpha
\newcommand{\al}[1]{\alpha_{#1}}

We define the output vector as an 11 dimensional vector $\rvec{v} = \{t, v, d, a, p_0, p_1, p_2, p_3, p_4, p_5, p_6\}$ where $t$, $v$, $d$, and $a$ represent tempo, velocity, deviation, and articulation respectively, and $p_i$ represents a single component of the pedal. For a predicted output vector $\vn$ and the target output vector $\vh$, standard MSE loss is $MSE(\vn, \vh) = \frac{1}{n}\sum_{i=1}^{n}(\vn_i - \vh_i)^2$. This can also be re-written as $MSE(\vn, \vh) = \frac{1}{11}[\df{t} + \df{v} + \df{d} + \df{a} + \sum_{i=1}^{7}(\vn_{p_{i}} - \vh_{p_{i}})^2]$. We introduce 5 different weight values: $\al{t}, \al{v}, \al{d}, \al{a}$ and $\al{p}$. Our weighted MSE loss is defined as $W_{MSE}(\vn, \vh) = \frac{1}{\al{t} + \al{v} + \al{d} + \al{a} + \al{p}}[\al{t}\df{t} + \al{v}\df{v} + \al{d}\df{d} + \al{a}\df{a} + \al{p}\sum_{i=1}^{7}(\vn_{p_{i}} - \vh_{p_{i}})^2 ]$. The original MSE can be seen as the weighted MSE with $\al{t}, \al{v}, \al{d}, \al{a} = 1$, and $\al{p} = 7$. 

According to our formulation of the loss, the pedal's optimization is weighted much higher than every other parameter for the original \vnetf{} loss. Is this is the right way to conceptualize what a 'good' performance is? Would a different configuration of the expressive feature weights lead to a better outcome? Answering these questions is non-trivial. Therefore, we call to question the validity of using MSE as an evaluation metric, especially as presented in the \vnetf{}. With this in mind, we ran additional experiments changing the weights for each expressive parameter to see what the effects on performance would be according to our evaluation. 


\section{Qualitative Evaluation}\label{sec:qualitative-analysis}
Our first general observation given our new loss evaluation is that the two most important factors for overall performance are tempo and pedal. Models that don't perform either of these two features within certain bounds correctly make performances unlistenable. Suppose a performance's global tempo is too fast, and every other expressive parameter renders correctly. In that case, the resulting performance will still sound bad enough that it's not worth listening to at all \footnote{See \href{https://ui.neptune.ai/richt3211/thesis/e/THESIS-86/artifacts}{\tm{86} Fantaisie Impromptu} and \href{https://ui.neptune.ai/richt3211/thesis/e/THESIS-126/artifacts}{\tm{126} Etude Op. 10 No. 12}.}. We noticed a similar phenomenon with the pedal. Some models generated performances with the sustain pedal applied at all times with very few breaks. The result is a performance that sounds "muddied" and unrefined. Although these performances are more bearable than those with extreme tempo, they are still hard to listen to in any meaningful way \footnote{See \href{https://ui.neptune.ai/richt3211/thesis/e/THESIS-125/artifacts}{\tm{125} Piano Sonata 11} }. 

% lm for lstm model
\newcommand{\lm}[1]{$LSTM_{N_{#1}}$}

We also notice that the tempo and timing of the Transformer models are more dynamic than the LSTM models, both for our LSTM baseline and the \vnet{} models. For some models, the variability in timing seemed to be a good thing, while for others, it was so bad that it almost sounded like the model was still "learning" how to play. The tempo for all LSTM based models (except for some slight variations in the performances from HAN-M \footnote{Performances for the HAN-M are available at \href{https://ui.neptune.ai/richt3211/thesis/e/THESIS-162/artifacts}{$N_{126}$}}) was extraordinarily consistent and non-changing to the point of sounding robotic and mundane. On one extreme with Transformer models, the highly dynamic tempo at times sounds like a real performer making mistakes \footnote{See \href{https://ui.neptune.ai/richt3211/thesis/e/THESIS-86/artifacts}{$T_{N_{86}}$ Piano Sonata 11}}. The other extreme is performances of some LSTM models that are so boring they don't sound "human" at all \footnote{See \href{https://ui.neptune.ai/richt3211/thesis/e/THESIS-123/artifacts}{\lm{123} Piano Sonata 11}}. 

The pedaling in general of all models was mediocre at best. This observation is consistent with that of~\citet{jeong2019virtuosonet}. There are models whose performance pedaling is much better than others and follows the music's natural cadence but still don't match pedal in actual human performance. Figure \ref{fig:pedal-difference} shows a visual comparison of the sustain pedal usage in different performances. 

\begin{figure}
    \centering
    \missingfigure{Images that show the difference of 3 performances, all of the same composition. One performance should have really bad pedal, another should have mediocre pedal, and the other (a human performance) should have natural pedal. Will be gathered with screenshots from Logic Pro}
    \caption{Test Caption}
    \label{fig:pedal-difference}
\end{figure}

The importance of tempo and pedal was part of the intuition that led to our formulation of the weighted MSE by expressive parameter defined in \ref{sec:qualitative-eval-problems}. Our first experiments with the weighted MSE used an even weight distribution. Because changing the loss function also changed our evaluation, we could not directly compare the models' quantitative results, and so no quantitative values are reported. All evaluation was by our qualitative listening test. We ran a few additional experiments with the tempo and pedal weighted high and some other changes in the model size. Table \ref{tab:qualitative-models} shows a full description of the models and their parameters.

% \newcommand{\nep}{$N_{id}$}
% \newcommand{\mn}{$M$} % mn for 'model name'
% \newcommand{\nl}{$L$} % nl: num layers
% \newcommand{\dhid}{$d_{hid}$} % dhid: dimension hidden size
% \newcommand{\drop}{$D$} % D: Dropout
% \newcommand{\lr}{$LR$} % LR: Learning Rate
% \newcommand{\clip}{$C$} % C: gradient clip
% \newcommand{\nh}{$H$} % nh: num heads
\newcommand{\am}{$AM$}

\begin{table}
    \setlength{\extrarowheight}{3pt}
    \begin{center}
    \begin{tabular}[]{| c | c c c c | c c c c c |}
        \hline
        \multicolumn{5}{|c|}{Model Configuration} & \multicolumn{5}{c|}{Expressive Weights}\\
        \hline
        \nep & \nl & \dhid & \nh & \am & $\al{t}$ & $\al{v}$ & $\al{d}$ & $\al{a}$ & $\al{p}$ \\ 
        \hline 
        150 & 256 & 6  & 6  & a & 1    & 1     & 1     & 1     & 7 \\
            &     &    &    & p & 1    & 1     & 1     & 1     & 7 \\
        154 &     &    &    &   & 0.2  & 0.2   & 0.2   & 0.2   & 0.2 \\
        156 &     &    &    &   & 0.33 & 0.11  & 0.11  & 0.11  & 0.33 \\
        157 &     &    &    &   & 0.4  & 0.067 & 0.067 & 0.067 & 0.4 \\
            &     &    &    & p & 0.4  & 0.067 & 0.067 & 0.067 & 0.4 \\
        159 & 528 & 12 & 13 &   & 0.4  & 0.067 & 0.067 & 0.067 & 0.4 \\
        \hline
    \end{tabular}
    \caption{The model configurations of additional experiments we ran after our initial quantitative evaluation effort. We show similar hyperparemters as in table \ref{tab:quantitative}, with an additional parameter \am which reprsents the articulation mask. A value of 'a' indicates that the articulation value was masked according to the note alignment, and a value of 'p' indicates that the articulation value was masked according to the pedal status. There are additional paramter values that are not present but are used in table \ref{tab:quantitative}: \lr{} is 0.0003, \clip{} is 0.5, and \drop{} is 0.1} 
    \label{tab:qualitative-models}
    \end{center}
\end{table}

\tm{154}, which weights all expressive parameters, generates performances with the global tempo slightly too fast and an extremely muddy pedal. \tm{150} which uses the original MSE loss with a higher pedal weight, produces better performances with reasonable pedaling. However, the tempo is inconsistent enough that the performance loses its cohesiveness as a whole. For these reasons, we increased both the tempo and pedal weights to be much higher than the others in models \tm{156} and \tm{157}. We found that the tempo and pedal weights \tm{156} were a bit too low - specifically, the pedal is almost just as muddy as it is in \tm{150} and the tempo is still a bit too fast, albeit more consistent and cohesive. We found that \tm{157} produced the best overall performance in the Transformer based models\footnote{Performances of Fantaisie Impromptu best demonstrate these differences. Compare \href{https://ui.neptune.ai/richt3211/thesis/e/THESIS-154/artifacts}{\tm{154}}, \href{https://ui.neptune.ai/richt3211/thesis/e/THESIS-150/artifacts}{\tm{150}}, \href{https://ui.neptune.ai/richt3211/thesis/e/THESIS-156/artifacts}{\tm{156}}, and \href{https://ui.neptune.ai/richt3211/thesis/e/THESIS-157/artifacts}{\tm{157}} }. It is likely that were we to continue to experiment with a different configuration of the weights that we could come up with even better results. 

Our last general observation is that the \vnet{} model HAN-M produces the best overall performances. In general, we feel that the tempo for HAN-M is a little too slow, but it still creates the most natural expression. This expression is most apparent in its performance of Beethoven's Piano Sonata 17, whose introduction leaves a large space for interpretation to achieve the desired listening result - rendering the score precisely as it is written results in an uninteresting performance. We found that the only model that made this performance "interesting" was the HAN-M. Although we have previously emphasized the problem using the existing quantitative metric to evaluate our models, our quantitative and qualitative evaluation of the HAN-M model indicates that it is the "best" model. The proposed Transformer architecture does not improve upon existing models. We will provide some intuition about why this is and possible model improvements for future work in section \rtodo{Add ref to discussion}. 





